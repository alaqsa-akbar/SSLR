{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341d08b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  raw_data_dir: C:\\Users\\engmr\\Downloads\\public_si_dat\n",
      "  train_csv: C:\\Users\\engmr\\Downloads\\public_si_dat\\train.csv\n",
      "  dev_csv: C:\\Users\\engmr\\Downloads\\public_si_dat\\dev.csv\n",
      "  pickle_file: C:\\Users\\engmr\\Downloads\\public_si_dat\\pose_data_isharah1000_hands_lips_body_May12.pkl\n",
      "  output_dir: isharah_clean\n",
      "  features_dir: isharah_clean/features\n",
      "  vocab_file: isharah_clean/vocab.json\n",
      "  labels_dir: isharah_clean/labels\n",
      "  models_dir: isharah_clean/models\n",
      "  d_model: 256\n",
      "  nhead: 4\n",
      "  num_encoder_layers: 4\n",
      "  num_decoder_layers: 4\n",
      "  dim_feedforward: 1024\n",
      "  dropout: 0.3\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.0003\n",
      "  num_epochs: 50\n",
      "  device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Raw data paths\n",
    "    \"raw_data_dir\": r\"C:\\Users\\engmr\\Downloads\\public_si_dat\",\n",
    "    \"train_csv\": r\"C:\\Users\\engmr\\Downloads\\public_si_dat\\train.csv\",\n",
    "    \"dev_csv\": r\"C:\\Users\\engmr\\Downloads\\public_si_dat\\dev.csv\",\n",
    "    \"pickle_file\": r\"C:\\Users\\engmr\\Downloads\\public_si_dat\\pose_data_isharah1000_hands_lips_body_May12.pkl\",\n",
    "    \n",
    "    # Output paths\n",
    "    \"output_dir\": \"isharah_clean\",\n",
    "    \"features_dir\": \"isharah_clean/features\",\n",
    "    \"vocab_file\": \"isharah_clean/vocab.json\",\n",
    "    \"labels_dir\": \"isharah_clean/labels\",\n",
    "    \"models_dir\": \"isharah_clean/models\",\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    \"d_model\": 256,\n",
    "    \"nhead\": 4,\n",
    "    \"num_encoder_layers\": 4,\n",
    "    \"num_decoder_layers\": 4,\n",
    "    \"dim_feedforward\": 1024,\n",
    "    \"dropout\": 0.3,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"num_epochs\": 50,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for key in [\"output_dir\", \"features_dir\", \"labels_dir\", \"models_dir\"]:\n",
    "    os.makedirs(CONFIG[key], exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4bcb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing normalization...\n",
      "Input shape: (10, 86, 2)\n",
      "Output shape: (10, 172)\n",
      "Output dtype: float32\n",
      "Output range: [-3.22, 3.23]\n"
     ]
    }
   ],
   "source": [
    "def normalize_keypoints(keypoints):\n",
    "    \"\"\"\n",
    "    Normalize keypoints to standard scale.\n",
    "    \n",
    "    Args:\n",
    "        keypoints: (T, 86, 2) array of (x, y) coordinates\n",
    "    \n",
    "    Returns:\n",
    "        (T, 172) flattened normalized array\n",
    "    \"\"\"\n",
    "    arr = np.array(keypoints, dtype=np.float32)\n",
    "    \n",
    "    if arr.ndim != 3 or arr.shape[2] != 2:\n",
    "        raise ValueError(f\"Expected shape (T, 86, 2), got {arr.shape}\")\n",
    "    \n",
    "    T, num_kps, _ = arr.shape\n",
    "    \n",
    "    # Center: subtract mean across all keypoints and frames\n",
    "    center = np.nanmean(arr.reshape(-1, 2), axis=0)\n",
    "    arr = arr - center\n",
    "    \n",
    "    # Scale: use median of per-frame standard deviations\n",
    "    per_frame_std = np.nanstd(arr, axis=1)  # (T, 2)\n",
    "    median_std = np.nanmedian(per_frame_std, axis=0)  # (2,)\n",
    "    scale = max(1e-6, np.mean(median_std))\n",
    "    arr = arr / scale\n",
    "    \n",
    "    # Flatten to (T, 172)\n",
    "    flattened = arr.reshape(T, -1)\n",
    "    \n",
    "    return flattened.astype(np.float32)\n",
    "\n",
    "\n",
    "# Test normalization\n",
    "print(\"\\nTesting normalization...\")\n",
    "test_kps = np.random.randn(10, 86, 2) * 100 + 500  # Random keypoints\n",
    "normalized = normalize_keypoints(test_kps)\n",
    "print(f\"Input shape: {test_kps.shape}\")\n",
    "print(f\"Output shape: {normalized.shape}\")\n",
    "print(f\"Output dtype: {normalized.dtype}\")\n",
    "print(f\"Output range: [{normalized.min():.2f}, {normalized.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820167ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Vocabulary size: 686\n",
      "Special tokens: <pad>=0, <sos>=1, <eos>=2\n",
      "Sample tokens: ['ا', 'اب', 'ابتسامه', 'ابن', 'ابها', 'ابيض', 'اتصال']\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(train_csv, dev_csv):\n",
    "    \"\"\"Build vocabulary from gloss texts.\"\"\"\n",
    "    all_tokens = set()\n",
    "    \n",
    "    # Process train\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    for gloss in df_train[\"gloss\"].astype(str):\n",
    "        tokens = gloss.strip().split()\n",
    "        all_tokens.update(tokens)\n",
    "    \n",
    "    # Process dev\n",
    "    df_dev = pd.read_csv(dev_csv)\n",
    "    for gloss in df_dev[\"gloss\"].astype(str):\n",
    "        tokens = gloss.strip().split()\n",
    "        all_tokens.update(tokens)\n",
    "    \n",
    "    # Create vocabulary with special tokens\n",
    "    vocab = [\"<pad>\", \"<sos>\", \"<eos>\"] + sorted(list(all_tokens))\n",
    "    vocab_map = {token: idx for idx, token in enumerate(vocab)}\n",
    "    \n",
    "    return vocab, vocab_map\n",
    "\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "vocab, vocab_map = build_vocabulary(CONFIG[\"train_csv\"], CONFIG[\"dev_csv\"])\n",
    "\n",
    "# Save vocabulary\n",
    "vocab_data = {\"vocab\": vocab, \"vocab_map\": vocab_map}\n",
    "with open(CONFIG[\"vocab_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Special tokens: <pad>={vocab_map['<pad>']}, <sos>={vocab_map['<sos>']}, <eos>={vocab_map['<eos>']}\")\n",
    "print(f\"Sample tokens: {vocab[3:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fca194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle file...\n",
      "Loaded 10450 samples from pickle\n",
      "\n",
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:21<00:00, 463.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ train: Processed 9500 samples, skipped 500\n",
      "\n",
      "Processing dev split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 949/949 [00:02<00:00, 379.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ dev: Processed 949 samples, skipped 0\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "Train samples: 9500\n",
      "Dev samples: 949\n",
      "Feature dimension: 172 (86 keypoints × 2 coordinates)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_split(csv_file, pickle_data, vocab_map, split_name):\n",
    "    \"\"\"\n",
    "    Process one split (train/dev) and save features + labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    features_dir = Path(CONFIG[\"features_dir\"]) / split_name\n",
    "    features_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    processed_samples = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"\\nProcessing {split_name} split...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        sample_id = str(row[\"id\"])\n",
    "        gloss = str(row[\"gloss\"]).strip()\n",
    "        \n",
    "        # Check if sample exists in pickle\n",
    "        if sample_id not in pickle_data:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get keypoints\n",
    "            keypoints = pickle_data[sample_id][\"keypoints\"]\n",
    "            \n",
    "            # Normalize\n",
    "            features = normalize_keypoints(keypoints)\n",
    "            \n",
    "            # Save features as numpy array\n",
    "            feat_path = features_dir / f\"{sample_id}.npy\"\n",
    "            np.save(feat_path, features)\n",
    "            \n",
    "            # Create target indices: [<sos>, token1, token2, ..., <eos>]\n",
    "            tokens = gloss.split()\n",
    "            target_idx = [vocab_map[\"<sos>\"]]\n",
    "            target_idx.extend([vocab_map[t] for t in tokens if t in vocab_map])\n",
    "            target_idx.append(vocab_map[\"<eos>\"])\n",
    "            \n",
    "            # Store sample info\n",
    "            processed_samples.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"gloss\": gloss,\n",
    "                \"target_idx\": json.dumps(target_idx),\n",
    "                \"num_frames\": len(features),\n",
    "                \"num_tokens\": len(target_idx)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sample_id}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    # Save labels CSV\n",
    "    labels_df = pd.DataFrame(processed_samples)\n",
    "    labels_path = Path(CONFIG[\"labels_dir\"]) / f\"{split_name}_labels.csv\"\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "    \n",
    "    print(f\"✓ {split_name}: Processed {len(processed_samples)} samples, skipped {skipped}\")\n",
    "    return labels_df\n",
    "\n",
    "\n",
    "# Load pickle file\n",
    "print(\"Loading pickle file...\")\n",
    "with open(CONFIG[\"pickle_file\"], \"rb\") as f:\n",
    "    pickle_data = pickle.load(f)\n",
    "print(f\"Loaded {len(pickle_data)} samples from pickle\")\n",
    "\n",
    "# Process both splits\n",
    "train_labels = process_split(CONFIG[\"train_csv\"], pickle_data, vocab_map, \"train\")\n",
    "dev_labels = process_split(CONFIG[\"dev_csv\"], pickle_data, vocab_map, \"dev\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train samples: {len(train_labels)}\")\n",
    "print(f\"Dev samples: {len(dev_labels)}\")\n",
    "print(f\"Feature dimension: 172 (86 keypoints × 2 coordinates)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268163f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageDataset(Dataset):\n",
    "    \"\"\"Dataset for sign language recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, features_dir, labels_csv, vocab_json):\n",
    "        self.features_dir = Path(features_dir)\n",
    "        \n",
    "        # Load labels\n",
    "        self.labels = pd.read_csv(labels_csv)\n",
    "        \n",
    "        # Load vocabulary\n",
    "        with open(vocab_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab_data = json.load(f)\n",
    "        self.vocab = vocab_data[\"vocab\"]\n",
    "        self.vocab_map = vocab_data[\"vocab_map\"]\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_idx = self.vocab_map[\"<pad>\"]\n",
    "        self.sos_idx = self.vocab_map[\"<sos>\"]\n",
    "        self.eos_idx = self.vocab_map[\"<eos>\"]\n",
    "        \n",
    "        print(f\"Loaded {len(self.labels)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        sample_id = str(row[\"sample_id\"])\n",
    "        \n",
    "        # Load features\n",
    "        feat_path = self.features_dir / f\"{sample_id}.npy\"\n",
    "        features = np.load(feat_path)\n",
    "        \n",
    "        # Load target\n",
    "        target = np.array(json.loads(row[\"target_idx\"]), dtype=np.int64)\n",
    "        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"target\": target,\n",
    "            \"sample_id\": sample_id\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function with padding.\"\"\"\n",
    "    # Sort by length\n",
    "    batch = sorted(batch, key=lambda x: len(x[\"features\"]), reverse=True)\n",
    "    \n",
    "    # Get max lengths\n",
    "    max_src_len = max(len(b[\"features\"]) for b in batch)\n",
    "    max_tgt_len = max(len(b[\"target\"]) for b in batch)\n",
    "    feat_dim = batch[0][\"features\"].shape[1]\n",
    "    \n",
    "    # Pad features\n",
    "    src_padded = np.zeros((len(batch), max_src_len, feat_dim), dtype=np.float32)\n",
    "    src_lengths = []\n",
    "    for i, b in enumerate(batch):\n",
    "        seq_len = len(b[\"features\"])\n",
    "        src_padded[i, :seq_len, :] = b[\"features\"]\n",
    "        src_lengths.append(seq_len)\n",
    "    \n",
    "    # Pad targets\n",
    "    tgt_padded = np.full((len(batch), max_tgt_len), -100, dtype=np.int64)\n",
    "    tgt_lengths = []\n",
    "    for i, b in enumerate(batch):\n",
    "        tgt_len = len(b[\"target\"])\n",
    "        tgt_padded[i, :tgt_len] = b[\"target\"]\n",
    "        tgt_lengths.append(tgt_len)\n",
    "    \n",
    "    return {\n",
    "        \"src\": torch.from_numpy(src_padded),\n",
    "        \"src_lengths\": torch.tensor(src_lengths, dtype=torch.long),\n",
    "        \"tgt\": torch.from_numpy(tgt_padded),\n",
    "        \"tgt_lengths\": torch.tensor(tgt_lengths, dtype=torch.long),\n",
    "        \"sample_ids\": [b[\"sample_id\"] for b in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94319f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"Sequence-to-sequence transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, src_feature_dim, tgt_vocab_size, d_model=256, nhead=4,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024,\n",
    "                 dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Source embedding (project features to d_model)\n",
    "        self.src_embedding = nn.Linear(src_feature_dim, d_model)\n",
    "        \n",
    "        # Target embedding\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_idx)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # Embed and encode\n",
    "        src = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Causal mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "        # Transform\n",
    "        output = self.transformer(\n",
    "            src=src,\n",
    "            tgt=tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project\n",
    "        logits = self.output_proj(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b38295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        src = batch[\"src\"].to(device)\n",
    "        src_lengths = batch[\"src_lengths\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "        \n",
    "        # Create masks\n",
    "        src_key_padding_mask = (\n",
    "            torch.arange(src.size(1), device=device).unsqueeze(0) >= src_lengths.unsqueeze(1)\n",
    "        )\n",
    "        \n",
    "        # Teacher forcing: input vs output\n",
    "        tgt_input = tgt[:, :-1].clone()\n",
    "        tgt_output = tgt[:, 1:].clone()\n",
    "        tgt_input[tgt_input == -100] = model.pad_idx\n",
    "        tgt_key_padding_mask = (tgt[:, :-1] == -100)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_input, src_key_padding_mask, tgt_key_padding_mask)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        num_tokens = (tgt_output != -100).sum().item()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch} | Batch {batch_idx+1}/{len(dataloader)} | \"\n",
    "                  f\"Loss: {total_loss/total_tokens:.4f} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch[\"src\"].to(device)\n",
    "            src_lengths = batch[\"src_lengths\"].to(device)\n",
    "            tgt = batch[\"tgt\"].to(device)\n",
    "            \n",
    "            src_key_padding_mask = (\n",
    "                torch.arange(src.size(1), device=device).unsqueeze(0) >= src_lengths.unsqueeze(1)\n",
    "            )\n",
    "            \n",
    "            tgt_input = tgt[:, :-1].clone()\n",
    "            tgt_output = tgt[:, 1:].clone()\n",
    "            tgt_input[tgt_input == -100] = model.pad_idx\n",
    "            tgt_key_padding_mask = (tgt[:, :-1] == -100)\n",
    "            \n",
    "            logits = model(src, tgt_input, src_key_padding_mask, tgt_key_padding_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
    "            \n",
    "            num_tokens = (tgt_output != -100).sum().item()\n",
    "            total_loss += loss.item() * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0129ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loaded 9500 samples\n",
      "Loaded 949 samples\n",
      "Train batches: 297\n",
      "Dev batches: 30\n",
      "\n",
      "Creating model...\n",
      "Model parameters: 7,770,030\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "train_dataset = SignLanguageDataset(\n",
    "    CONFIG[\"features_dir\"] + \"/train\",\n",
    "    CONFIG[\"labels_dir\"] + \"/train_labels.csv\",\n",
    "    CONFIG[\"vocab_file\"]\n",
    ")\n",
    "dev_dataset = SignLanguageDataset(\n",
    "    CONFIG[\"features_dir\"] + \"/dev\",\n",
    "    CONFIG[\"labels_dir\"] + \"/dev_labels.csv\",\n",
    "    CONFIG[\"vocab_file\"]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=True, collate_fn=collate_fn, num_workers=0\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset, batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False, collate_fn=collate_fn, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Dev batches: {len(dev_loader)}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating model...\")\n",
    "model = Seq2SeqTransformer(\n",
    "    src_feature_dim=172,\n",
    "    tgt_vocab_size=len(train_dataset.vocab),\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    nhead=CONFIG[\"nhead\"],\n",
    "    num_encoder_layers=CONFIG[\"num_encoder_layers\"],\n",
    "    num_decoder_layers=CONFIG[\"num_decoder_layers\"],\n",
    "    dim_feedforward=CONFIG[\"dim_feedforward\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    "    pad_idx=train_dataset.pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886606ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmr\\AppData\\Local\\Temp\\ipykernel_7676\\2301793643.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{CONFIG['models_dir']}/best_model.pt\")\n",
      "C:\\Users\\engmr\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 10/297 | Loss: 0.1297 | Time: 4.1s\n",
      "Epoch 1 | Batch 20/297 | Loss: 0.1291 | Time: 8.0s\n",
      "Epoch 1 | Batch 30/297 | Loss: 0.1237 | Time: 11.7s\n",
      "Epoch 1 | Batch 40/297 | Loss: 0.1210 | Time: 15.5s\n",
      "Epoch 1 | Batch 50/297 | Loss: 0.1169 | Time: 19.3s\n",
      "Epoch 1 | Batch 60/297 | Loss: 0.1196 | Time: 23.0s\n",
      "Epoch 1 | Batch 70/297 | Loss: 0.1214 | Time: 26.9s\n",
      "Epoch 1 | Batch 80/297 | Loss: 0.1211 | Time: 30.7s\n",
      "Epoch 1 | Batch 90/297 | Loss: 0.1203 | Time: 34.8s\n",
      "Epoch 1 | Batch 100/297 | Loss: 0.1210 | Time: 38.9s\n",
      "Epoch 1 | Batch 110/297 | Loss: 0.1187 | Time: 42.8s\n",
      "Epoch 1 | Batch 120/297 | Loss: 0.1179 | Time: 46.5s\n",
      "Epoch 1 | Batch 130/297 | Loss: 0.1176 | Time: 50.3s\n",
      "Epoch 1 | Batch 140/297 | Loss: 0.1188 | Time: 54.3s\n",
      "Epoch 1 | Batch 150/297 | Loss: 0.1190 | Time: 58.3s\n",
      "Epoch 1 | Batch 160/297 | Loss: 0.1199 | Time: 62.0s\n",
      "Epoch 1 | Batch 170/297 | Loss: 0.1203 | Time: 65.8s\n",
      "Epoch 1 | Batch 180/297 | Loss: 0.1203 | Time: 69.6s\n",
      "Epoch 1 | Batch 190/297 | Loss: 0.1203 | Time: 73.3s\n",
      "Epoch 1 | Batch 200/297 | Loss: 0.1202 | Time: 77.0s\n",
      "Epoch 1 | Batch 210/297 | Loss: 0.1201 | Time: 81.1s\n",
      "Epoch 1 | Batch 220/297 | Loss: 0.1204 | Time: 85.3s\n",
      "Epoch 1 | Batch 230/297 | Loss: 0.1197 | Time: 89.4s\n",
      "Epoch 1 | Batch 240/297 | Loss: 0.1201 | Time: 93.5s\n",
      "Epoch 1 | Batch 250/297 | Loss: 0.1201 | Time: 97.5s\n",
      "Epoch 1 | Batch 260/297 | Loss: 0.1201 | Time: 101.4s\n",
      "Epoch 1 | Batch 270/297 | Loss: 0.1199 | Time: 105.4s\n",
      "Epoch 1 | Batch 280/297 | Loss: 0.1198 | Time: 109.2s\n",
      "Epoch 1 | Batch 290/297 | Loss: 0.1208 | Time: 113.1s\n",
      "Train Loss: 0.1205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4153\n",
      "Learning Rate: 0.000300\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 2/50\n",
      "------------------------------------------------------------\n",
      "Epoch 2 | Batch 10/297 | Loss: 0.0922 | Time: 2.1s\n",
      "Epoch 2 | Batch 20/297 | Loss: 0.0924 | Time: 4.0s\n",
      "Epoch 2 | Batch 30/297 | Loss: 0.0997 | Time: 5.7s\n",
      "Epoch 2 | Batch 40/297 | Loss: 0.1057 | Time: 7.4s\n",
      "Epoch 2 | Batch 50/297 | Loss: 0.1102 | Time: 9.2s\n",
      "Epoch 2 | Batch 60/297 | Loss: 0.1096 | Time: 11.0s\n",
      "Epoch 2 | Batch 70/297 | Loss: 0.1117 | Time: 12.8s\n",
      "Epoch 2 | Batch 80/297 | Loss: 0.1123 | Time: 14.5s\n",
      "Epoch 2 | Batch 90/297 | Loss: 0.1098 | Time: 16.2s\n",
      "Epoch 2 | Batch 100/297 | Loss: 0.1107 | Time: 18.0s\n",
      "Epoch 2 | Batch 110/297 | Loss: 0.1105 | Time: 19.7s\n",
      "Epoch 2 | Batch 120/297 | Loss: 0.1097 | Time: 21.2s\n",
      "Epoch 2 | Batch 130/297 | Loss: 0.1094 | Time: 22.8s\n",
      "Epoch 2 | Batch 140/297 | Loss: 0.1102 | Time: 24.9s\n",
      "Epoch 2 | Batch 150/297 | Loss: 0.1122 | Time: 26.6s\n",
      "Epoch 2 | Batch 160/297 | Loss: 0.1128 | Time: 28.3s\n",
      "Epoch 2 | Batch 170/297 | Loss: 0.1143 | Time: 30.1s\n",
      "Epoch 2 | Batch 180/297 | Loss: 0.1149 | Time: 31.7s\n",
      "Epoch 2 | Batch 190/297 | Loss: 0.1145 | Time: 33.7s\n",
      "Epoch 2 | Batch 200/297 | Loss: 0.1143 | Time: 35.4s\n",
      "Epoch 2 | Batch 210/297 | Loss: 0.1140 | Time: 37.2s\n",
      "Epoch 2 | Batch 220/297 | Loss: 0.1137 | Time: 38.9s\n",
      "Epoch 2 | Batch 230/297 | Loss: 0.1138 | Time: 40.7s\n",
      "Epoch 2 | Batch 240/297 | Loss: 0.1156 | Time: 42.4s\n",
      "Epoch 2 | Batch 250/297 | Loss: 0.1163 | Time: 44.1s\n",
      "Epoch 2 | Batch 260/297 | Loss: 0.1161 | Time: 46.1s\n",
      "Epoch 2 | Batch 270/297 | Loss: 0.1167 | Time: 48.2s\n",
      "Epoch 2 | Batch 280/297 | Loss: 0.1160 | Time: 49.9s\n",
      "Epoch 2 | Batch 290/297 | Loss: 0.1171 | Time: 51.6s\n",
      "Train Loss: 0.1173\n",
      "Val Loss: 0.3918\n",
      "Learning Rate: 0.000300\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 3/50\n",
      "------------------------------------------------------------\n",
      "Epoch 3 | Batch 10/297 | Loss: 0.1010 | Time: 1.8s\n",
      "Epoch 3 | Batch 20/297 | Loss: 0.1086 | Time: 3.8s\n",
      "Epoch 3 | Batch 30/297 | Loss: 0.1042 | Time: 5.7s\n",
      "Epoch 3 | Batch 40/297 | Loss: 0.1020 | Time: 7.4s\n",
      "Epoch 3 | Batch 50/297 | Loss: 0.1021 | Time: 9.6s\n",
      "Epoch 3 | Batch 60/297 | Loss: 0.1039 | Time: 11.2s\n",
      "Epoch 3 | Batch 70/297 | Loss: 0.1057 | Time: 13.0s\n",
      "Epoch 3 | Batch 80/297 | Loss: 0.1081 | Time: 14.9s\n",
      "Epoch 3 | Batch 90/297 | Loss: 0.1082 | Time: 16.5s\n",
      "Epoch 3 | Batch 100/297 | Loss: 0.1095 | Time: 18.1s\n",
      "Epoch 3 | Batch 110/297 | Loss: 0.1107 | Time: 19.7s\n",
      "Epoch 3 | Batch 120/297 | Loss: 0.1109 | Time: 21.5s\n",
      "Epoch 3 | Batch 130/297 | Loss: 0.1107 | Time: 23.3s\n",
      "Epoch 3 | Batch 140/297 | Loss: 0.1117 | Time: 25.0s\n",
      "Epoch 3 | Batch 150/297 | Loss: 0.1132 | Time: 26.7s\n",
      "Epoch 3 | Batch 160/297 | Loss: 0.1142 | Time: 28.5s\n",
      "Epoch 3 | Batch 170/297 | Loss: 0.1144 | Time: 30.2s\n",
      "Epoch 3 | Batch 180/297 | Loss: 0.1134 | Time: 31.9s\n",
      "Epoch 3 | Batch 190/297 | Loss: 0.1134 | Time: 33.5s\n",
      "Epoch 3 | Batch 200/297 | Loss: 0.1132 | Time: 35.4s\n",
      "Epoch 3 | Batch 210/297 | Loss: 0.1130 | Time: 37.1s\n",
      "Epoch 3 | Batch 220/297 | Loss: 0.1137 | Time: 38.9s\n",
      "Epoch 3 | Batch 230/297 | Loss: 0.1142 | Time: 40.5s\n",
      "Epoch 3 | Batch 240/297 | Loss: 0.1142 | Time: 42.2s\n",
      "Epoch 3 | Batch 250/297 | Loss: 0.1135 | Time: 44.0s\n",
      "Epoch 3 | Batch 260/297 | Loss: 0.1146 | Time: 45.7s\n",
      "Epoch 3 | Batch 270/297 | Loss: 0.1150 | Time: 47.4s\n",
      "Epoch 3 | Batch 280/297 | Loss: 0.1155 | Time: 49.2s\n",
      "Epoch 3 | Batch 290/297 | Loss: 0.1163 | Time: 51.1s\n",
      "Train Loss: 0.1161\n",
      "Val Loss: 0.3811\n",
      "Learning Rate: 0.000300\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 4/50\n",
      "------------------------------------------------------------\n",
      "Epoch 4 | Batch 10/297 | Loss: 0.0873 | Time: 1.8s\n",
      "Epoch 4 | Batch 20/297 | Loss: 0.0905 | Time: 3.5s\n",
      "Epoch 4 | Batch 30/297 | Loss: 0.0952 | Time: 5.4s\n",
      "Epoch 4 | Batch 40/297 | Loss: 0.0885 | Time: 7.2s\n",
      "Epoch 4 | Batch 50/297 | Loss: 0.0930 | Time: 8.9s\n",
      "Epoch 4 | Batch 60/297 | Loss: 0.0959 | Time: 10.7s\n",
      "Epoch 4 | Batch 70/297 | Loss: 0.0989 | Time: 12.5s\n",
      "Epoch 4 | Batch 80/297 | Loss: 0.0993 | Time: 14.1s\n",
      "Epoch 4 | Batch 90/297 | Loss: 0.1005 | Time: 15.9s\n",
      "Epoch 4 | Batch 100/297 | Loss: 0.1028 | Time: 17.8s\n",
      "Epoch 4 | Batch 110/297 | Loss: 0.1037 | Time: 19.6s\n",
      "Epoch 4 | Batch 120/297 | Loss: 0.1052 | Time: 21.4s\n",
      "Epoch 4 | Batch 130/297 | Loss: 0.1060 | Time: 23.5s\n",
      "Epoch 4 | Batch 140/297 | Loss: 0.1073 | Time: 25.3s\n",
      "Epoch 4 | Batch 150/297 | Loss: 0.1066 | Time: 27.0s\n",
      "Epoch 4 | Batch 160/297 | Loss: 0.1072 | Time: 28.7s\n",
      "Epoch 4 | Batch 170/297 | Loss: 0.1076 | Time: 30.5s\n",
      "Epoch 4 | Batch 180/297 | Loss: 0.1081 | Time: 32.4s\n",
      "Epoch 4 | Batch 190/297 | Loss: 0.1100 | Time: 34.2s\n",
      "Epoch 4 | Batch 200/297 | Loss: 0.1115 | Time: 36.0s\n",
      "Epoch 4 | Batch 210/297 | Loss: 0.1121 | Time: 37.7s\n",
      "Epoch 4 | Batch 220/297 | Loss: 0.1122 | Time: 39.4s\n",
      "Epoch 4 | Batch 230/297 | Loss: 0.1115 | Time: 41.2s\n",
      "Epoch 4 | Batch 240/297 | Loss: 0.1117 | Time: 42.8s\n",
      "Epoch 4 | Batch 250/297 | Loss: 0.1126 | Time: 44.5s\n",
      "Epoch 4 | Batch 260/297 | Loss: 0.1137 | Time: 46.3s\n",
      "Epoch 4 | Batch 270/297 | Loss: 0.1134 | Time: 48.0s\n",
      "Epoch 4 | Batch 280/297 | Loss: 0.1137 | Time: 49.9s\n",
      "Epoch 4 | Batch 290/297 | Loss: 0.1141 | Time: 51.8s\n",
      "Train Loss: 0.1145\n",
      "Val Loss: 0.4399\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 5/50\n",
      "------------------------------------------------------------\n",
      "Epoch 5 | Batch 10/297 | Loss: 0.0950 | Time: 1.6s\n",
      "Epoch 5 | Batch 20/297 | Loss: 0.0859 | Time: 3.5s\n",
      "Epoch 5 | Batch 30/297 | Loss: 0.0872 | Time: 5.3s\n",
      "Epoch 5 | Batch 40/297 | Loss: 0.0866 | Time: 7.1s\n",
      "Epoch 5 | Batch 50/297 | Loss: 0.0865 | Time: 8.8s\n",
      "Epoch 5 | Batch 60/297 | Loss: 0.0875 | Time: 10.6s\n",
      "Epoch 5 | Batch 70/297 | Loss: 0.0904 | Time: 12.3s\n",
      "Epoch 5 | Batch 80/297 | Loss: 0.0894 | Time: 13.9s\n",
      "Epoch 5 | Batch 90/297 | Loss: 0.0923 | Time: 15.9s\n",
      "Epoch 5 | Batch 100/297 | Loss: 0.0931 | Time: 17.7s\n",
      "Epoch 5 | Batch 110/297 | Loss: 0.0965 | Time: 19.6s\n",
      "Epoch 5 | Batch 120/297 | Loss: 0.0988 | Time: 21.4s\n",
      "Epoch 5 | Batch 130/297 | Loss: 0.0993 | Time: 23.4s\n",
      "Epoch 5 | Batch 140/297 | Loss: 0.0995 | Time: 25.1s\n",
      "Epoch 5 | Batch 150/297 | Loss: 0.1001 | Time: 26.9s\n",
      "Epoch 5 | Batch 160/297 | Loss: 0.1003 | Time: 28.4s\n",
      "Epoch 5 | Batch 170/297 | Loss: 0.1006 | Time: 30.1s\n",
      "Epoch 5 | Batch 180/297 | Loss: 0.1012 | Time: 31.9s\n",
      "Epoch 5 | Batch 190/297 | Loss: 0.1013 | Time: 33.8s\n",
      "Epoch 5 | Batch 200/297 | Loss: 0.1021 | Time: 35.6s\n",
      "Epoch 5 | Batch 210/297 | Loss: 0.1021 | Time: 37.3s\n",
      "Epoch 5 | Batch 220/297 | Loss: 0.1031 | Time: 38.9s\n",
      "Epoch 5 | Batch 230/297 | Loss: 0.1038 | Time: 40.7s\n",
      "Epoch 5 | Batch 240/297 | Loss: 0.1045 | Time: 42.5s\n",
      "Epoch 5 | Batch 250/297 | Loss: 0.1046 | Time: 44.2s\n",
      "Epoch 5 | Batch 260/297 | Loss: 0.1042 | Time: 46.0s\n",
      "Epoch 5 | Batch 270/297 | Loss: 0.1041 | Time: 47.9s\n",
      "Epoch 5 | Batch 280/297 | Loss: 0.1042 | Time: 49.5s\n",
      "Epoch 5 | Batch 290/297 | Loss: 0.1047 | Time: 51.2s\n",
      "Train Loss: 0.1048\n",
      "Val Loss: 0.3918\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 6/50\n",
      "------------------------------------------------------------\n",
      "Epoch 6 | Batch 10/297 | Loss: 0.1061 | Time: 1.7s\n",
      "Epoch 6 | Batch 20/297 | Loss: 0.1143 | Time: 3.5s\n",
      "Epoch 6 | Batch 30/297 | Loss: 0.1094 | Time: 5.2s\n",
      "Epoch 6 | Batch 40/297 | Loss: 0.1055 | Time: 7.1s\n",
      "Epoch 6 | Batch 50/297 | Loss: 0.1032 | Time: 8.9s\n",
      "Epoch 6 | Batch 60/297 | Loss: 0.1037 | Time: 10.5s\n",
      "Epoch 6 | Batch 70/297 | Loss: 0.1033 | Time: 12.5s\n",
      "Epoch 6 | Batch 80/297 | Loss: 0.1028 | Time: 14.1s\n",
      "Epoch 6 | Batch 90/297 | Loss: 0.1030 | Time: 15.9s\n",
      "Epoch 6 | Batch 100/297 | Loss: 0.1050 | Time: 17.5s\n",
      "Epoch 6 | Batch 110/297 | Loss: 0.1039 | Time: 19.5s\n",
      "Epoch 6 | Batch 120/297 | Loss: 0.1058 | Time: 21.2s\n",
      "Epoch 6 | Batch 130/297 | Loss: 0.1059 | Time: 23.1s\n",
      "Epoch 6 | Batch 140/297 | Loss: 0.1058 | Time: 25.0s\n",
      "Epoch 6 | Batch 150/297 | Loss: 0.1063 | Time: 26.7s\n",
      "Epoch 6 | Batch 160/297 | Loss: 0.1043 | Time: 28.6s\n",
      "Epoch 6 | Batch 170/297 | Loss: 0.1054 | Time: 30.2s\n",
      "Epoch 6 | Batch 180/297 | Loss: 0.1046 | Time: 32.1s\n",
      "Epoch 6 | Batch 190/297 | Loss: 0.1042 | Time: 33.9s\n",
      "Epoch 6 | Batch 200/297 | Loss: 0.1047 | Time: 35.7s\n",
      "Epoch 6 | Batch 210/297 | Loss: 0.1061 | Time: 37.7s\n",
      "Epoch 6 | Batch 220/297 | Loss: 0.1064 | Time: 39.4s\n",
      "Epoch 6 | Batch 230/297 | Loss: 0.1067 | Time: 41.0s\n",
      "Epoch 6 | Batch 240/297 | Loss: 0.1075 | Time: 42.7s\n",
      "Epoch 6 | Batch 250/297 | Loss: 0.1080 | Time: 44.5s\n",
      "Epoch 6 | Batch 260/297 | Loss: 0.1085 | Time: 46.1s\n",
      "Epoch 6 | Batch 270/297 | Loss: 0.1086 | Time: 47.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Batch 280/297 | Loss: 0.1090 | Time: 49.6s\n",
      "Epoch 6 | Batch 290/297 | Loss: 0.1093 | Time: 51.3s\n",
      "Train Loss: 0.1086\n",
      "Val Loss: 0.3927\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 7/50\n",
      "------------------------------------------------------------\n",
      "Epoch 7 | Batch 10/297 | Loss: 0.0826 | Time: 1.7s\n",
      "Epoch 7 | Batch 20/297 | Loss: 0.0757 | Time: 3.4s\n",
      "Epoch 7 | Batch 30/297 | Loss: 0.0690 | Time: 5.2s\n",
      "Epoch 7 | Batch 40/297 | Loss: 0.0691 | Time: 6.8s\n",
      "Epoch 7 | Batch 50/297 | Loss: 0.0693 | Time: 8.7s\n",
      "Epoch 7 | Batch 60/297 | Loss: 0.0670 | Time: 10.4s\n",
      "Epoch 7 | Batch 70/297 | Loss: 0.0656 | Time: 12.3s\n",
      "Epoch 7 | Batch 80/297 | Loss: 0.0646 | Time: 14.1s\n",
      "Epoch 7 | Batch 90/297 | Loss: 0.0628 | Time: 15.7s\n",
      "Epoch 7 | Batch 100/297 | Loss: 0.0627 | Time: 17.3s\n",
      "Epoch 7 | Batch 110/297 | Loss: 0.0621 | Time: 19.0s\n",
      "Epoch 7 | Batch 120/297 | Loss: 0.0618 | Time: 20.9s\n",
      "Epoch 7 | Batch 130/297 | Loss: 0.0608 | Time: 22.6s\n",
      "Epoch 7 | Batch 140/297 | Loss: 0.0602 | Time: 24.4s\n",
      "Epoch 7 | Batch 150/297 | Loss: 0.0588 | Time: 26.2s\n",
      "Epoch 7 | Batch 160/297 | Loss: 0.0581 | Time: 28.0s\n",
      "Epoch 7 | Batch 170/297 | Loss: 0.0582 | Time: 29.5s\n",
      "Epoch 7 | Batch 180/297 | Loss: 0.0575 | Time: 31.6s\n",
      "Epoch 7 | Batch 190/297 | Loss: 0.0572 | Time: 33.5s\n",
      "Epoch 7 | Batch 200/297 | Loss: 0.0577 | Time: 35.3s\n",
      "Epoch 7 | Batch 210/297 | Loss: 0.0575 | Time: 37.1s\n",
      "Epoch 7 | Batch 220/297 | Loss: 0.0569 | Time: 38.8s\n",
      "Epoch 7 | Batch 230/297 | Loss: 0.0564 | Time: 40.6s\n",
      "Epoch 7 | Batch 240/297 | Loss: 0.0566 | Time: 42.5s\n",
      "Epoch 7 | Batch 250/297 | Loss: 0.0566 | Time: 44.0s\n",
      "Epoch 7 | Batch 260/297 | Loss: 0.0557 | Time: 45.9s\n",
      "Epoch 7 | Batch 270/297 | Loss: 0.0550 | Time: 47.7s\n",
      "Epoch 7 | Batch 280/297 | Loss: 0.0549 | Time: 49.4s\n",
      "Epoch 7 | Batch 290/297 | Loss: 0.0551 | Time: 51.2s\n",
      "Train Loss: 0.0551\n",
      "Val Loss: 0.3438\n",
      "Learning Rate: 0.000150\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 8/50\n",
      "------------------------------------------------------------\n",
      "Epoch 8 | Batch 10/297 | Loss: 0.0423 | Time: 1.8s\n",
      "Epoch 8 | Batch 20/297 | Loss: 0.0450 | Time: 3.4s\n",
      "Epoch 8 | Batch 30/297 | Loss: 0.0450 | Time: 5.2s\n",
      "Epoch 8 | Batch 40/297 | Loss: 0.0460 | Time: 7.1s\n",
      "Epoch 8 | Batch 50/297 | Loss: 0.0406 | Time: 8.8s\n",
      "Epoch 8 | Batch 60/297 | Loss: 0.0420 | Time: 10.6s\n",
      "Epoch 8 | Batch 70/297 | Loss: 0.0410 | Time: 12.4s\n",
      "Epoch 8 | Batch 80/297 | Loss: 0.0400 | Time: 14.2s\n",
      "Epoch 8 | Batch 90/297 | Loss: 0.0400 | Time: 16.0s\n",
      "Epoch 8 | Batch 100/297 | Loss: 0.0397 | Time: 17.8s\n",
      "Epoch 8 | Batch 110/297 | Loss: 0.0391 | Time: 19.7s\n",
      "Epoch 8 | Batch 120/297 | Loss: 0.0396 | Time: 21.4s\n",
      "Epoch 8 | Batch 130/297 | Loss: 0.0393 | Time: 23.1s\n",
      "Epoch 8 | Batch 140/297 | Loss: 0.0397 | Time: 24.8s\n",
      "Epoch 8 | Batch 150/297 | Loss: 0.0408 | Time: 26.5s\n",
      "Epoch 8 | Batch 160/297 | Loss: 0.0405 | Time: 28.2s\n",
      "Epoch 8 | Batch 170/297 | Loss: 0.0413 | Time: 29.9s\n",
      "Epoch 8 | Batch 180/297 | Loss: 0.0412 | Time: 31.8s\n",
      "Epoch 8 | Batch 190/297 | Loss: 0.0410 | Time: 33.5s\n",
      "Epoch 8 | Batch 200/297 | Loss: 0.0407 | Time: 35.5s\n",
      "Epoch 8 | Batch 210/297 | Loss: 0.0407 | Time: 37.2s\n",
      "Epoch 8 | Batch 220/297 | Loss: 0.0411 | Time: 38.9s\n",
      "Epoch 8 | Batch 230/297 | Loss: 0.0411 | Time: 40.6s\n",
      "Epoch 8 | Batch 240/297 | Loss: 0.0424 | Time: 42.4s\n",
      "Epoch 8 | Batch 250/297 | Loss: 0.0422 | Time: 44.2s\n",
      "Epoch 8 | Batch 260/297 | Loss: 0.0426 | Time: 45.8s\n",
      "Epoch 8 | Batch 270/297 | Loss: 0.0429 | Time: 47.8s\n",
      "Epoch 8 | Batch 280/297 | Loss: 0.0426 | Time: 49.9s\n",
      "Epoch 8 | Batch 290/297 | Loss: 0.0426 | Time: 51.6s\n",
      "Train Loss: 0.0424\n",
      "Val Loss: 0.3529\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 9/50\n",
      "------------------------------------------------------------\n",
      "Epoch 9 | Batch 10/297 | Loss: 0.0440 | Time: 1.8s\n",
      "Epoch 9 | Batch 20/297 | Loss: 0.0428 | Time: 3.7s\n",
      "Epoch 9 | Batch 30/297 | Loss: 0.0401 | Time: 5.5s\n",
      "Epoch 9 | Batch 40/297 | Loss: 0.0372 | Time: 7.3s\n",
      "Epoch 9 | Batch 50/297 | Loss: 0.0375 | Time: 9.1s\n",
      "Epoch 9 | Batch 60/297 | Loss: 0.0373 | Time: 10.9s\n",
      "Epoch 9 | Batch 70/297 | Loss: 0.0374 | Time: 12.7s\n",
      "Epoch 9 | Batch 80/297 | Loss: 0.0377 | Time: 14.3s\n",
      "Epoch 9 | Batch 90/297 | Loss: 0.0375 | Time: 15.9s\n",
      "Epoch 9 | Batch 100/297 | Loss: 0.0374 | Time: 17.9s\n",
      "Epoch 9 | Batch 110/297 | Loss: 0.0377 | Time: 19.5s\n",
      "Epoch 9 | Batch 120/297 | Loss: 0.0384 | Time: 21.3s\n",
      "Epoch 9 | Batch 130/297 | Loss: 0.0377 | Time: 23.0s\n",
      "Epoch 9 | Batch 140/297 | Loss: 0.0371 | Time: 24.7s\n",
      "Epoch 9 | Batch 150/297 | Loss: 0.0366 | Time: 26.5s\n",
      "Epoch 9 | Batch 160/297 | Loss: 0.0361 | Time: 28.3s\n",
      "Epoch 9 | Batch 170/297 | Loss: 0.0360 | Time: 30.2s\n",
      "Epoch 9 | Batch 180/297 | Loss: 0.0362 | Time: 32.0s\n",
      "Epoch 9 | Batch 190/297 | Loss: 0.0364 | Time: 33.8s\n",
      "Epoch 9 | Batch 200/297 | Loss: 0.0371 | Time: 35.6s\n",
      "Epoch 9 | Batch 210/297 | Loss: 0.0376 | Time: 37.4s\n",
      "Epoch 9 | Batch 220/297 | Loss: 0.0377 | Time: 39.3s\n",
      "Epoch 9 | Batch 230/297 | Loss: 0.0383 | Time: 41.2s\n",
      "Epoch 9 | Batch 240/297 | Loss: 0.0389 | Time: 43.0s\n",
      "Epoch 9 | Batch 250/297 | Loss: 0.0392 | Time: 44.7s\n",
      "Epoch 9 | Batch 260/297 | Loss: 0.0391 | Time: 46.5s\n",
      "Epoch 9 | Batch 270/297 | Loss: 0.0391 | Time: 48.3s\n",
      "Epoch 9 | Batch 280/297 | Loss: 0.0392 | Time: 50.0s\n",
      "Epoch 9 | Batch 290/297 | Loss: 0.0386 | Time: 51.7s\n",
      "Train Loss: 0.0383\n",
      "Val Loss: 0.3253\n",
      "Learning Rate: 0.000150\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 10/50\n",
      "------------------------------------------------------------\n",
      "Epoch 10 | Batch 10/297 | Loss: 0.0336 | Time: 1.7s\n",
      "Epoch 10 | Batch 20/297 | Loss: 0.0359 | Time: 3.5s\n",
      "Epoch 10 | Batch 30/297 | Loss: 0.0326 | Time: 5.3s\n",
      "Epoch 10 | Batch 40/297 | Loss: 0.0345 | Time: 7.2s\n",
      "Epoch 10 | Batch 50/297 | Loss: 0.0345 | Time: 9.0s\n",
      "Epoch 10 | Batch 60/297 | Loss: 0.0349 | Time: 10.7s\n",
      "Epoch 10 | Batch 70/297 | Loss: 0.0353 | Time: 12.6s\n",
      "Epoch 10 | Batch 80/297 | Loss: 0.0346 | Time: 14.5s\n",
      "Epoch 10 | Batch 90/297 | Loss: 0.0351 | Time: 16.3s\n",
      "Epoch 10 | Batch 100/297 | Loss: 0.0344 | Time: 18.0s\n",
      "Epoch 10 | Batch 110/297 | Loss: 0.0342 | Time: 19.8s\n",
      "Epoch 10 | Batch 120/297 | Loss: 0.0348 | Time: 21.5s\n",
      "Epoch 10 | Batch 130/297 | Loss: 0.0340 | Time: 23.2s\n",
      "Epoch 10 | Batch 140/297 | Loss: 0.0338 | Time: 25.1s\n",
      "Epoch 10 | Batch 150/297 | Loss: 0.0337 | Time: 26.8s\n",
      "Epoch 10 | Batch 160/297 | Loss: 0.0335 | Time: 28.7s\n",
      "Epoch 10 | Batch 170/297 | Loss: 0.0336 | Time: 30.6s\n",
      "Epoch 10 | Batch 180/297 | Loss: 0.0335 | Time: 32.3s\n",
      "Epoch 10 | Batch 190/297 | Loss: 0.0336 | Time: 34.2s\n",
      "Epoch 10 | Batch 200/297 | Loss: 0.0338 | Time: 36.0s\n",
      "Epoch 10 | Batch 210/297 | Loss: 0.0335 | Time: 37.9s\n",
      "Epoch 10 | Batch 220/297 | Loss: 0.0337 | Time: 39.6s\n",
      "Epoch 10 | Batch 230/297 | Loss: 0.0339 | Time: 41.0s\n",
      "Epoch 10 | Batch 240/297 | Loss: 0.0340 | Time: 42.7s\n",
      "Epoch 10 | Batch 250/297 | Loss: 0.0341 | Time: 44.5s\n",
      "Epoch 10 | Batch 260/297 | Loss: 0.0342 | Time: 46.2s\n",
      "Epoch 10 | Batch 270/297 | Loss: 0.0344 | Time: 48.1s\n",
      "Epoch 10 | Batch 280/297 | Loss: 0.0345 | Time: 50.0s\n",
      "Epoch 10 | Batch 290/297 | Loss: 0.0343 | Time: 51.7s\n",
      "Train Loss: 0.0345\n",
      "Val Loss: 0.3711\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 11/50\n",
      "------------------------------------------------------------\n",
      "Epoch 11 | Batch 10/297 | Loss: 0.0236 | Time: 1.8s\n",
      "Epoch 11 | Batch 20/297 | Loss: 0.0215 | Time: 3.6s\n",
      "Epoch 11 | Batch 30/297 | Loss: 0.0261 | Time: 5.6s\n",
      "Epoch 11 | Batch 40/297 | Loss: 0.0268 | Time: 7.5s\n",
      "Epoch 11 | Batch 50/297 | Loss: 0.0263 | Time: 9.3s\n",
      "Epoch 11 | Batch 60/297 | Loss: 0.0268 | Time: 11.0s\n",
      "Epoch 11 | Batch 70/297 | Loss: 0.0269 | Time: 12.7s\n",
      "Epoch 11 | Batch 80/297 | Loss: 0.0277 | Time: 14.5s\n",
      "Epoch 11 | Batch 90/297 | Loss: 0.0282 | Time: 16.1s\n",
      "Epoch 11 | Batch 100/297 | Loss: 0.0291 | Time: 17.8s\n",
      "Epoch 11 | Batch 110/297 | Loss: 0.0295 | Time: 19.7s\n",
      "Epoch 11 | Batch 120/297 | Loss: 0.0293 | Time: 21.4s\n",
      "Epoch 11 | Batch 130/297 | Loss: 0.0296 | Time: 23.2s\n",
      "Epoch 11 | Batch 140/297 | Loss: 0.0305 | Time: 24.8s\n",
      "Epoch 11 | Batch 150/297 | Loss: 0.0313 | Time: 26.8s\n",
      "Epoch 11 | Batch 160/297 | Loss: 0.0320 | Time: 28.6s\n",
      "Epoch 11 | Batch 170/297 | Loss: 0.0319 | Time: 30.3s\n",
      "Epoch 11 | Batch 180/297 | Loss: 0.0323 | Time: 32.2s\n",
      "Epoch 11 | Batch 190/297 | Loss: 0.0324 | Time: 33.9s\n",
      "Epoch 11 | Batch 200/297 | Loss: 0.0322 | Time: 35.5s\n",
      "Epoch 11 | Batch 210/297 | Loss: 0.0319 | Time: 37.6s\n",
      "Epoch 11 | Batch 220/297 | Loss: 0.0324 | Time: 39.3s\n",
      "Epoch 11 | Batch 230/297 | Loss: 0.0322 | Time: 41.2s\n",
      "Epoch 11 | Batch 240/297 | Loss: 0.0324 | Time: 42.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Batch 250/297 | Loss: 0.0325 | Time: 44.6s\n",
      "Epoch 11 | Batch 260/297 | Loss: 0.0327 | Time: 46.2s\n",
      "Epoch 11 | Batch 270/297 | Loss: 0.0329 | Time: 48.1s\n",
      "Epoch 11 | Batch 280/297 | Loss: 0.0329 | Time: 49.9s\n",
      "Epoch 11 | Batch 290/297 | Loss: 0.0327 | Time: 51.6s\n",
      "Train Loss: 0.0331\n",
      "Val Loss: 0.3589\n",
      "Learning Rate: 0.000150\n",
      "\n",
      "Epoch 12/50\n",
      "------------------------------------------------------------\n",
      "Epoch 12 | Batch 10/297 | Loss: 0.0274 | Time: 1.7s\n",
      "Epoch 12 | Batch 20/297 | Loss: 0.0342 | Time: 3.3s\n",
      "Epoch 12 | Batch 30/297 | Loss: 0.0341 | Time: 5.3s\n",
      "Epoch 12 | Batch 40/297 | Loss: 0.0362 | Time: 7.1s\n",
      "Epoch 12 | Batch 50/297 | Loss: 0.0351 | Time: 9.1s\n",
      "Epoch 12 | Batch 60/297 | Loss: 0.0340 | Time: 10.9s\n",
      "Epoch 12 | Batch 70/297 | Loss: 0.0332 | Time: 12.6s\n",
      "Epoch 12 | Batch 80/297 | Loss: 0.0326 | Time: 14.3s\n",
      "Epoch 12 | Batch 90/297 | Loss: 0.0314 | Time: 16.0s\n",
      "Epoch 12 | Batch 100/297 | Loss: 0.0316 | Time: 17.8s\n",
      "Epoch 12 | Batch 110/297 | Loss: 0.0311 | Time: 19.5s\n",
      "Epoch 12 | Batch 120/297 | Loss: 0.0307 | Time: 21.1s\n",
      "Epoch 12 | Batch 130/297 | Loss: 0.0310 | Time: 22.8s\n",
      "Epoch 12 | Batch 140/297 | Loss: 0.0309 | Time: 24.6s\n",
      "Epoch 12 | Batch 150/297 | Loss: 0.0310 | Time: 26.3s\n",
      "Epoch 12 | Batch 160/297 | Loss: 0.0311 | Time: 28.2s\n",
      "Epoch 12 | Batch 170/297 | Loss: 0.0318 | Time: 30.0s\n",
      "Epoch 12 | Batch 180/297 | Loss: 0.0324 | Time: 31.8s\n",
      "Epoch 12 | Batch 190/297 | Loss: 0.0325 | Time: 33.5s\n",
      "Epoch 12 | Batch 200/297 | Loss: 0.0330 | Time: 35.2s\n",
      "Epoch 12 | Batch 210/297 | Loss: 0.0331 | Time: 37.0s\n",
      "Epoch 12 | Batch 220/297 | Loss: 0.0331 | Time: 38.8s\n",
      "Epoch 12 | Batch 230/297 | Loss: 0.0330 | Time: 40.6s\n",
      "Epoch 12 | Batch 240/297 | Loss: 0.0328 | Time: 42.4s\n",
      "Epoch 12 | Batch 250/297 | Loss: 0.0327 | Time: 44.3s\n",
      "Epoch 12 | Batch 260/297 | Loss: 0.0327 | Time: 46.1s\n",
      "Epoch 12 | Batch 270/297 | Loss: 0.0334 | Time: 47.8s\n",
      "Epoch 12 | Batch 280/297 | Loss: 0.0337 | Time: 49.6s\n",
      "Epoch 12 | Batch 290/297 | Loss: 0.0336 | Time: 51.6s\n",
      "Train Loss: 0.0333\n",
      "Val Loss: 0.3406\n",
      "Learning Rate: 0.000075\n",
      "\n",
      "Epoch 13/50\n",
      "------------------------------------------------------------\n",
      "Epoch 13 | Batch 10/297 | Loss: 0.0312 | Time: 1.7s\n",
      "Epoch 13 | Batch 20/297 | Loss: 0.0260 | Time: 3.5s\n",
      "Epoch 13 | Batch 30/297 | Loss: 0.0234 | Time: 5.3s\n",
      "Epoch 13 | Batch 40/297 | Loss: 0.0232 | Time: 7.0s\n",
      "Epoch 13 | Batch 50/297 | Loss: 0.0213 | Time: 8.7s\n",
      "Epoch 13 | Batch 60/297 | Loss: 0.0210 | Time: 10.6s\n",
      "Epoch 13 | Batch 70/297 | Loss: 0.0203 | Time: 12.5s\n",
      "Epoch 13 | Batch 80/297 | Loss: 0.0196 | Time: 14.4s\n",
      "Epoch 13 | Batch 90/297 | Loss: 0.0200 | Time: 16.0s\n",
      "Epoch 13 | Batch 100/297 | Loss: 0.0201 | Time: 17.7s\n",
      "Epoch 13 | Batch 110/297 | Loss: 0.0201 | Time: 19.3s\n",
      "Epoch 13 | Batch 120/297 | Loss: 0.0195 | Time: 20.9s\n",
      "Epoch 13 | Batch 130/297 | Loss: 0.0194 | Time: 22.8s\n",
      "Epoch 13 | Batch 140/297 | Loss: 0.0197 | Time: 24.8s\n",
      "Epoch 13 | Batch 150/297 | Loss: 0.0196 | Time: 26.4s\n",
      "Epoch 13 | Batch 160/297 | Loss: 0.0193 | Time: 28.2s\n",
      "Epoch 13 | Batch 170/297 | Loss: 0.0190 | Time: 30.0s\n",
      "Epoch 13 | Batch 180/297 | Loss: 0.0192 | Time: 31.7s\n",
      "Epoch 13 | Batch 190/297 | Loss: 0.0191 | Time: 33.6s\n",
      "Epoch 13 | Batch 200/297 | Loss: 0.0188 | Time: 35.5s\n",
      "Epoch 13 | Batch 210/297 | Loss: 0.0185 | Time: 37.1s\n",
      "Epoch 13 | Batch 220/297 | Loss: 0.0185 | Time: 38.9s\n",
      "Epoch 13 | Batch 230/297 | Loss: 0.0184 | Time: 40.9s\n",
      "Epoch 13 | Batch 240/297 | Loss: 0.0184 | Time: 42.5s\n",
      "Epoch 13 | Batch 250/297 | Loss: 0.0184 | Time: 44.3s\n",
      "Epoch 13 | Batch 260/297 | Loss: 0.0184 | Time: 46.4s\n",
      "Epoch 13 | Batch 270/297 | Loss: 0.0182 | Time: 48.2s\n",
      "Epoch 13 | Batch 280/297 | Loss: 0.0181 | Time: 50.0s\n",
      "Epoch 13 | Batch 290/297 | Loss: 0.0180 | Time: 51.7s\n",
      "Train Loss: 0.0180\n",
      "Val Loss: 0.3219\n",
      "Learning Rate: 0.000075\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 14/50\n",
      "------------------------------------------------------------\n",
      "Epoch 14 | Batch 10/297 | Loss: 0.0143 | Time: 1.9s\n",
      "Epoch 14 | Batch 20/297 | Loss: 0.0161 | Time: 3.8s\n",
      "Epoch 14 | Batch 30/297 | Loss: 0.0139 | Time: 5.6s\n",
      "Epoch 14 | Batch 40/297 | Loss: 0.0137 | Time: 7.4s\n",
      "Epoch 14 | Batch 50/297 | Loss: 0.0139 | Time: 9.1s\n",
      "Epoch 14 | Batch 60/297 | Loss: 0.0136 | Time: 10.9s\n",
      "Epoch 14 | Batch 70/297 | Loss: 0.0137 | Time: 12.5s\n",
      "Epoch 14 | Batch 80/297 | Loss: 0.0137 | Time: 14.2s\n",
      "Epoch 14 | Batch 90/297 | Loss: 0.0143 | Time: 15.9s\n",
      "Epoch 14 | Batch 100/297 | Loss: 0.0144 | Time: 17.8s\n",
      "Epoch 14 | Batch 110/297 | Loss: 0.0145 | Time: 19.7s\n",
      "Epoch 14 | Batch 120/297 | Loss: 0.0149 | Time: 21.4s\n",
      "Epoch 14 | Batch 130/297 | Loss: 0.0146 | Time: 23.2s\n",
      "Epoch 14 | Batch 140/297 | Loss: 0.0145 | Time: 24.9s\n",
      "Epoch 14 | Batch 150/297 | Loss: 0.0146 | Time: 26.9s\n",
      "Epoch 14 | Batch 160/297 | Loss: 0.0143 | Time: 28.7s\n",
      "Epoch 14 | Batch 170/297 | Loss: 0.0142 | Time: 30.5s\n",
      "Epoch 14 | Batch 180/297 | Loss: 0.0143 | Time: 32.1s\n",
      "Epoch 14 | Batch 190/297 | Loss: 0.0145 | Time: 33.8s\n",
      "Epoch 14 | Batch 200/297 | Loss: 0.0144 | Time: 35.5s\n",
      "Epoch 14 | Batch 210/297 | Loss: 0.0142 | Time: 37.3s\n",
      "Epoch 14 | Batch 220/297 | Loss: 0.0142 | Time: 39.0s\n",
      "Epoch 14 | Batch 230/297 | Loss: 0.0141 | Time: 40.9s\n",
      "Epoch 14 | Batch 240/297 | Loss: 0.0140 | Time: 42.6s\n",
      "Epoch 14 | Batch 250/297 | Loss: 0.0138 | Time: 44.6s\n",
      "Epoch 14 | Batch 260/297 | Loss: 0.0138 | Time: 46.3s\n",
      "Epoch 14 | Batch 270/297 | Loss: 0.0137 | Time: 47.9s\n",
      "Epoch 14 | Batch 280/297 | Loss: 0.0135 | Time: 49.5s\n",
      "Epoch 14 | Batch 290/297 | Loss: 0.0134 | Time: 51.3s\n",
      "Train Loss: 0.0134\n",
      "Val Loss: 0.3389\n",
      "Learning Rate: 0.000075\n",
      "\n",
      "Epoch 15/50\n",
      "------------------------------------------------------------\n",
      "Epoch 15 | Batch 10/297 | Loss: 0.0095 | Time: 1.8s\n",
      "Epoch 15 | Batch 20/297 | Loss: 0.0097 | Time: 3.5s\n",
      "Epoch 15 | Batch 30/297 | Loss: 0.0103 | Time: 5.1s\n",
      "Epoch 15 | Batch 40/297 | Loss: 0.0101 | Time: 6.8s\n",
      "Epoch 15 | Batch 50/297 | Loss: 0.0104 | Time: 8.6s\n",
      "Epoch 15 | Batch 60/297 | Loss: 0.0098 | Time: 10.3s\n",
      "Epoch 15 | Batch 70/297 | Loss: 0.0095 | Time: 12.0s\n",
      "Epoch 15 | Batch 80/297 | Loss: 0.0098 | Time: 13.7s\n",
      "Epoch 15 | Batch 90/297 | Loss: 0.0102 | Time: 15.3s\n",
      "Epoch 15 | Batch 100/297 | Loss: 0.0106 | Time: 16.9s\n",
      "Epoch 15 | Batch 110/297 | Loss: 0.0105 | Time: 18.9s\n",
      "Epoch 15 | Batch 120/297 | Loss: 0.0110 | Time: 20.8s\n",
      "Epoch 15 | Batch 130/297 | Loss: 0.0111 | Time: 22.4s\n",
      "Epoch 15 | Batch 140/297 | Loss: 0.0117 | Time: 24.3s\n",
      "Epoch 15 | Batch 150/297 | Loss: 0.0117 | Time: 26.2s\n",
      "Epoch 15 | Batch 160/297 | Loss: 0.0117 | Time: 27.9s\n",
      "Epoch 15 | Batch 170/297 | Loss: 0.0116 | Time: 29.6s\n",
      "Epoch 15 | Batch 180/297 | Loss: 0.0117 | Time: 31.2s\n",
      "Epoch 15 | Batch 190/297 | Loss: 0.0116 | Time: 33.0s\n",
      "Epoch 15 | Batch 200/297 | Loss: 0.0117 | Time: 34.8s\n",
      "Epoch 15 | Batch 210/297 | Loss: 0.0118 | Time: 36.6s\n",
      "Epoch 15 | Batch 220/297 | Loss: 0.0117 | Time: 38.5s\n",
      "Epoch 15 | Batch 230/297 | Loss: 0.0119 | Time: 40.3s\n",
      "Epoch 15 | Batch 240/297 | Loss: 0.0119 | Time: 42.0s\n",
      "Epoch 15 | Batch 250/297 | Loss: 0.0119 | Time: 43.8s\n",
      "Epoch 15 | Batch 260/297 | Loss: 0.0121 | Time: 45.7s\n",
      "Epoch 15 | Batch 270/297 | Loss: 0.0121 | Time: 47.7s\n",
      "Epoch 15 | Batch 280/297 | Loss: 0.0119 | Time: 49.4s\n",
      "Epoch 15 | Batch 290/297 | Loss: 0.0120 | Time: 51.2s\n",
      "Train Loss: 0.0120\n",
      "Val Loss: 0.3350\n",
      "Learning Rate: 0.000075\n",
      "\n",
      "Epoch 16/50\n",
      "------------------------------------------------------------\n",
      "Epoch 16 | Batch 10/297 | Loss: 0.0094 | Time: 1.8s\n",
      "Epoch 16 | Batch 20/297 | Loss: 0.0090 | Time: 3.6s\n",
      "Epoch 16 | Batch 30/297 | Loss: 0.0085 | Time: 5.6s\n",
      "Epoch 16 | Batch 40/297 | Loss: 0.0089 | Time: 7.4s\n",
      "Epoch 16 | Batch 50/297 | Loss: 0.0090 | Time: 9.2s\n",
      "Epoch 16 | Batch 60/297 | Loss: 0.0087 | Time: 11.0s\n",
      "Epoch 16 | Batch 70/297 | Loss: 0.0092 | Time: 13.0s\n",
      "Epoch 16 | Batch 80/297 | Loss: 0.0089 | Time: 14.9s\n",
      "Epoch 16 | Batch 90/297 | Loss: 0.0093 | Time: 16.6s\n",
      "Epoch 16 | Batch 100/297 | Loss: 0.0095 | Time: 18.3s\n",
      "Epoch 16 | Batch 110/297 | Loss: 0.0095 | Time: 20.1s\n",
      "Epoch 16 | Batch 120/297 | Loss: 0.0095 | Time: 22.1s\n",
      "Epoch 16 | Batch 130/297 | Loss: 0.0101 | Time: 23.7s\n",
      "Epoch 16 | Batch 140/297 | Loss: 0.0104 | Time: 25.4s\n",
      "Epoch 16 | Batch 150/297 | Loss: 0.0102 | Time: 27.3s\n",
      "Epoch 16 | Batch 160/297 | Loss: 0.0104 | Time: 29.0s\n",
      "Epoch 16 | Batch 170/297 | Loss: 0.0106 | Time: 30.9s\n",
      "Epoch 16 | Batch 180/297 | Loss: 0.0104 | Time: 32.5s\n",
      "Epoch 16 | Batch 190/297 | Loss: 0.0106 | Time: 34.5s\n",
      "Epoch 16 | Batch 200/297 | Loss: 0.0108 | Time: 36.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Batch 210/297 | Loss: 0.0108 | Time: 37.9s\n",
      "Epoch 16 | Batch 220/297 | Loss: 0.0109 | Time: 39.6s\n",
      "Epoch 16 | Batch 230/297 | Loss: 0.0110 | Time: 41.4s\n",
      "Epoch 16 | Batch 240/297 | Loss: 0.0115 | Time: 43.2s\n",
      "Epoch 16 | Batch 250/297 | Loss: 0.0116 | Time: 45.1s\n",
      "Epoch 16 | Batch 260/297 | Loss: 0.0114 | Time: 46.8s\n",
      "Epoch 16 | Batch 270/297 | Loss: 0.0114 | Time: 48.4s\n",
      "Epoch 16 | Batch 280/297 | Loss: 0.0114 | Time: 50.3s\n",
      "Epoch 16 | Batch 290/297 | Loss: 0.0113 | Time: 52.2s\n",
      "Train Loss: 0.0112\n",
      "Val Loss: 0.3559\n",
      "Learning Rate: 0.000037\n",
      "\n",
      "Epoch 17/50\n",
      "------------------------------------------------------------\n",
      "Epoch 17 | Batch 10/297 | Loss: 0.0111 | Time: 1.7s\n",
      "Epoch 17 | Batch 20/297 | Loss: 0.0126 | Time: 3.5s\n",
      "Epoch 17 | Batch 30/297 | Loss: 0.0101 | Time: 5.3s\n",
      "Epoch 17 | Batch 40/297 | Loss: 0.0108 | Time: 6.9s\n",
      "Epoch 17 | Batch 50/297 | Loss: 0.0110 | Time: 8.5s\n",
      "Epoch 17 | Batch 60/297 | Loss: 0.0107 | Time: 10.3s\n",
      "Epoch 17 | Batch 70/297 | Loss: 0.0112 | Time: 11.9s\n",
      "Epoch 17 | Batch 80/297 | Loss: 0.0112 | Time: 13.6s\n",
      "Epoch 17 | Batch 90/297 | Loss: 0.0106 | Time: 15.5s\n",
      "Epoch 17 | Batch 100/297 | Loss: 0.0105 | Time: 17.2s\n",
      "Epoch 17 | Batch 110/297 | Loss: 0.0105 | Time: 18.9s\n",
      "Epoch 17 | Batch 120/297 | Loss: 0.0100 | Time: 20.8s\n",
      "Epoch 17 | Batch 130/297 | Loss: 0.0097 | Time: 22.5s\n",
      "Epoch 17 | Batch 140/297 | Loss: 0.0096 | Time: 24.3s\n",
      "Epoch 17 | Batch 150/297 | Loss: 0.0098 | Time: 26.1s\n",
      "Epoch 17 | Batch 160/297 | Loss: 0.0099 | Time: 27.8s\n",
      "Epoch 17 | Batch 170/297 | Loss: 0.0099 | Time: 29.6s\n",
      "Epoch 17 | Batch 180/297 | Loss: 0.0098 | Time: 31.6s\n",
      "Epoch 17 | Batch 190/297 | Loss: 0.0096 | Time: 33.5s\n",
      "Epoch 17 | Batch 200/297 | Loss: 0.0097 | Time: 35.5s\n",
      "Epoch 17 | Batch 210/297 | Loss: 0.0096 | Time: 37.3s\n",
      "Epoch 17 | Batch 220/297 | Loss: 0.0095 | Time: 38.9s\n",
      "Epoch 17 | Batch 230/297 | Loss: 0.0093 | Time: 40.9s\n",
      "Epoch 17 | Batch 240/297 | Loss: 0.0093 | Time: 42.8s\n",
      "Epoch 17 | Batch 250/297 | Loss: 0.0092 | Time: 44.6s\n",
      "Epoch 17 | Batch 260/297 | Loss: 0.0092 | Time: 46.4s\n",
      "Epoch 17 | Batch 270/297 | Loss: 0.0091 | Time: 48.1s\n",
      "Epoch 17 | Batch 280/297 | Loss: 0.0092 | Time: 49.8s\n",
      "Epoch 17 | Batch 290/297 | Loss: 0.0090 | Time: 51.5s\n",
      "Train Loss: 0.0090\n",
      "Val Loss: 0.3325\n",
      "Learning Rate: 0.000037\n",
      "\n",
      "Epoch 18/50\n",
      "------------------------------------------------------------\n",
      "Epoch 18 | Batch 10/297 | Loss: 0.0069 | Time: 1.9s\n",
      "Epoch 18 | Batch 20/297 | Loss: 0.0072 | Time: 3.7s\n",
      "Epoch 18 | Batch 30/297 | Loss: 0.0074 | Time: 5.5s\n",
      "Epoch 18 | Batch 40/297 | Loss: 0.0078 | Time: 7.4s\n",
      "Epoch 18 | Batch 50/297 | Loss: 0.0084 | Time: 9.2s\n",
      "Epoch 18 | Batch 60/297 | Loss: 0.0082 | Time: 10.9s\n",
      "Epoch 18 | Batch 70/297 | Loss: 0.0082 | Time: 12.7s\n",
      "Epoch 18 | Batch 80/297 | Loss: 0.0082 | Time: 14.5s\n",
      "Epoch 18 | Batch 90/297 | Loss: 0.0078 | Time: 16.2s\n",
      "Epoch 18 | Batch 100/297 | Loss: 0.0079 | Time: 18.0s\n",
      "Epoch 18 | Batch 110/297 | Loss: 0.0078 | Time: 19.7s\n",
      "Epoch 18 | Batch 120/297 | Loss: 0.0077 | Time: 21.5s\n",
      "Epoch 18 | Batch 130/297 | Loss: 0.0077 | Time: 23.3s\n",
      "Epoch 18 | Batch 140/297 | Loss: 0.0076 | Time: 24.9s\n",
      "Epoch 18 | Batch 150/297 | Loss: 0.0075 | Time: 26.7s\n",
      "Epoch 18 | Batch 160/297 | Loss: 0.0077 | Time: 28.5s\n",
      "Epoch 18 | Batch 170/297 | Loss: 0.0075 | Time: 30.4s\n",
      "Epoch 18 | Batch 180/297 | Loss: 0.0074 | Time: 32.1s\n",
      "Epoch 18 | Batch 190/297 | Loss: 0.0074 | Time: 33.8s\n",
      "Epoch 18 | Batch 200/297 | Loss: 0.0074 | Time: 35.4s\n",
      "Epoch 18 | Batch 210/297 | Loss: 0.0074 | Time: 37.1s\n",
      "Epoch 18 | Batch 220/297 | Loss: 0.0074 | Time: 38.9s\n",
      "Epoch 18 | Batch 230/297 | Loss: 0.0073 | Time: 40.5s\n",
      "Epoch 18 | Batch 240/297 | Loss: 0.0073 | Time: 42.4s\n",
      "Epoch 18 | Batch 250/297 | Loss: 0.0073 | Time: 44.2s\n",
      "Epoch 18 | Batch 260/297 | Loss: 0.0073 | Time: 46.1s\n",
      "Epoch 18 | Batch 270/297 | Loss: 0.0074 | Time: 47.9s\n",
      "Epoch 18 | Batch 280/297 | Loss: 0.0073 | Time: 49.9s\n",
      "Epoch 18 | Batch 290/297 | Loss: 0.0073 | Time: 51.7s\n",
      "Train Loss: 0.0073\n",
      "Val Loss: 0.3303\n",
      "Learning Rate: 0.000037\n",
      "\n",
      "Epoch 19/50\n",
      "------------------------------------------------------------\n",
      "Epoch 19 | Batch 10/297 | Loss: 0.0032 | Time: 1.9s\n",
      "Epoch 19 | Batch 20/297 | Loss: 0.0051 | Time: 3.6s\n",
      "Epoch 19 | Batch 30/297 | Loss: 0.0060 | Time: 5.4s\n",
      "Epoch 19 | Batch 40/297 | Loss: 0.0058 | Time: 7.2s\n",
      "Epoch 19 | Batch 50/297 | Loss: 0.0055 | Time: 8.9s\n",
      "Epoch 19 | Batch 60/297 | Loss: 0.0058 | Time: 10.7s\n",
      "Epoch 19 | Batch 70/297 | Loss: 0.0057 | Time: 12.5s\n",
      "Epoch 19 | Batch 80/297 | Loss: 0.0059 | Time: 14.2s\n",
      "Epoch 19 | Batch 90/297 | Loss: 0.0059 | Time: 15.9s\n",
      "Epoch 19 | Batch 100/297 | Loss: 0.0059 | Time: 17.7s\n",
      "Epoch 19 | Batch 110/297 | Loss: 0.0057 | Time: 19.6s\n",
      "Epoch 19 | Batch 120/297 | Loss: 0.0058 | Time: 21.4s\n",
      "Epoch 19 | Batch 130/297 | Loss: 0.0059 | Time: 23.1s\n",
      "Epoch 19 | Batch 140/297 | Loss: 0.0059 | Time: 24.7s\n",
      "Epoch 19 | Batch 150/297 | Loss: 0.0059 | Time: 26.8s\n",
      "Epoch 19 | Batch 160/297 | Loss: 0.0059 | Time: 28.5s\n",
      "Epoch 19 | Batch 170/297 | Loss: 0.0058 | Time: 30.3s\n",
      "Epoch 19 | Batch 180/297 | Loss: 0.0057 | Time: 32.1s\n",
      "Epoch 19 | Batch 190/297 | Loss: 0.0056 | Time: 33.7s\n",
      "Epoch 19 | Batch 200/297 | Loss: 0.0057 | Time: 35.4s\n",
      "Epoch 19 | Batch 210/297 | Loss: 0.0057 | Time: 37.1s\n",
      "Epoch 19 | Batch 220/297 | Loss: 0.0057 | Time: 39.0s\n",
      "Epoch 19 | Batch 230/297 | Loss: 0.0057 | Time: 40.7s\n",
      "Epoch 19 | Batch 240/297 | Loss: 0.0058 | Time: 42.5s\n",
      "Epoch 19 | Batch 250/297 | Loss: 0.0058 | Time: 44.2s\n",
      "Epoch 19 | Batch 260/297 | Loss: 0.0059 | Time: 46.1s\n",
      "Epoch 19 | Batch 270/297 | Loss: 0.0060 | Time: 47.9s\n",
      "Epoch 19 | Batch 280/297 | Loss: 0.0061 | Time: 49.7s\n",
      "Epoch 19 | Batch 290/297 | Loss: 0.0062 | Time: 51.4s\n",
      "Train Loss: 0.0063\n",
      "Val Loss: 0.3243\n",
      "Learning Rate: 0.000019\n",
      "\n",
      "Epoch 20/50\n",
      "------------------------------------------------------------\n",
      "Epoch 20 | Batch 10/297 | Loss: 0.0035 | Time: 1.8s\n",
      "Epoch 20 | Batch 20/297 | Loss: 0.0055 | Time: 3.7s\n",
      "Epoch 20 | Batch 30/297 | Loss: 0.0062 | Time: 5.5s\n",
      "Epoch 20 | Batch 40/297 | Loss: 0.0061 | Time: 7.3s\n",
      "Epoch 20 | Batch 50/297 | Loss: 0.0060 | Time: 8.9s\n",
      "Epoch 20 | Batch 60/297 | Loss: 0.0060 | Time: 10.6s\n",
      "Epoch 20 | Batch 70/297 | Loss: 0.0058 | Time: 12.4s\n",
      "Epoch 20 | Batch 80/297 | Loss: 0.0057 | Time: 14.1s\n",
      "Epoch 20 | Batch 90/297 | Loss: 0.0055 | Time: 15.8s\n",
      "Epoch 20 | Batch 100/297 | Loss: 0.0054 | Time: 17.7s\n",
      "Epoch 20 | Batch 110/297 | Loss: 0.0053 | Time: 19.3s\n",
      "Epoch 20 | Batch 120/297 | Loss: 0.0053 | Time: 21.0s\n",
      "Epoch 20 | Batch 130/297 | Loss: 0.0056 | Time: 22.7s\n",
      "Epoch 20 | Batch 140/297 | Loss: 0.0056 | Time: 24.6s\n",
      "Epoch 20 | Batch 150/297 | Loss: 0.0056 | Time: 26.4s\n",
      "Epoch 20 | Batch 160/297 | Loss: 0.0057 | Time: 28.2s\n",
      "Epoch 20 | Batch 170/297 | Loss: 0.0055 | Time: 30.1s\n",
      "Epoch 20 | Batch 180/297 | Loss: 0.0056 | Time: 31.7s\n",
      "Epoch 20 | Batch 190/297 | Loss: 0.0055 | Time: 33.6s\n",
      "Epoch 20 | Batch 200/297 | Loss: 0.0056 | Time: 35.4s\n",
      "Epoch 20 | Batch 210/297 | Loss: 0.0055 | Time: 37.3s\n",
      "Epoch 20 | Batch 220/297 | Loss: 0.0054 | Time: 39.1s\n",
      "Epoch 20 | Batch 230/297 | Loss: 0.0055 | Time: 41.0s\n",
      "Epoch 20 | Batch 240/297 | Loss: 0.0055 | Time: 42.8s\n",
      "Epoch 20 | Batch 250/297 | Loss: 0.0054 | Time: 44.6s\n",
      "Epoch 20 | Batch 260/297 | Loss: 0.0055 | Time: 46.3s\n",
      "Epoch 20 | Batch 270/297 | Loss: 0.0055 | Time: 48.1s\n",
      "Epoch 20 | Batch 280/297 | Loss: 0.0054 | Time: 49.9s\n",
      "Epoch 20 | Batch 290/297 | Loss: 0.0054 | Time: 51.7s\n",
      "Train Loss: 0.0054\n",
      "Val Loss: 0.3227\n",
      "Learning Rate: 0.000019\n",
      "\n",
      "Epoch 21/50\n",
      "------------------------------------------------------------\n",
      "Epoch 21 | Batch 10/297 | Loss: 0.0064 | Time: 1.7s\n",
      "Epoch 21 | Batch 20/297 | Loss: 0.0051 | Time: 3.4s\n",
      "Epoch 21 | Batch 30/297 | Loss: 0.0057 | Time: 5.1s\n",
      "Epoch 21 | Batch 40/297 | Loss: 0.0060 | Time: 7.0s\n",
      "Epoch 21 | Batch 50/297 | Loss: 0.0054 | Time: 8.7s\n",
      "Epoch 21 | Batch 60/297 | Loss: 0.0057 | Time: 10.6s\n",
      "Epoch 21 | Batch 70/297 | Loss: 0.0060 | Time: 12.3s\n",
      "Epoch 21 | Batch 80/297 | Loss: 0.0056 | Time: 14.2s\n",
      "Epoch 21 | Batch 90/297 | Loss: 0.0054 | Time: 16.0s\n",
      "Epoch 21 | Batch 100/297 | Loss: 0.0052 | Time: 17.6s\n",
      "Epoch 21 | Batch 110/297 | Loss: 0.0052 | Time: 19.3s\n",
      "Epoch 21 | Batch 120/297 | Loss: 0.0050 | Time: 21.2s\n",
      "Epoch 21 | Batch 130/297 | Loss: 0.0051 | Time: 23.1s\n",
      "Epoch 21 | Batch 140/297 | Loss: 0.0050 | Time: 25.0s\n",
      "Epoch 21 | Batch 150/297 | Loss: 0.0049 | Time: 26.6s\n",
      "Epoch 21 | Batch 160/297 | Loss: 0.0048 | Time: 28.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Batch 170/297 | Loss: 0.0049 | Time: 30.2s\n",
      "Epoch 21 | Batch 180/297 | Loss: 0.0050 | Time: 31.9s\n",
      "Epoch 21 | Batch 190/297 | Loss: 0.0050 | Time: 34.0s\n",
      "Epoch 21 | Batch 200/297 | Loss: 0.0051 | Time: 35.6s\n",
      "Epoch 21 | Batch 210/297 | Loss: 0.0051 | Time: 37.5s\n",
      "Epoch 21 | Batch 220/297 | Loss: 0.0050 | Time: 39.3s\n",
      "Epoch 21 | Batch 230/297 | Loss: 0.0050 | Time: 41.0s\n",
      "Epoch 21 | Batch 240/297 | Loss: 0.0050 | Time: 42.7s\n",
      "Epoch 21 | Batch 250/297 | Loss: 0.0049 | Time: 44.5s\n",
      "Epoch 21 | Batch 260/297 | Loss: 0.0050 | Time: 46.3s\n",
      "Epoch 21 | Batch 270/297 | Loss: 0.0050 | Time: 48.0s\n",
      "Epoch 21 | Batch 280/297 | Loss: 0.0051 | Time: 49.9s\n",
      "Epoch 21 | Batch 290/297 | Loss: 0.0050 | Time: 51.7s\n",
      "Train Loss: 0.0050\n",
      "Val Loss: 0.3288\n",
      "Learning Rate: 0.000019\n",
      "\n",
      "Epoch 22/50\n",
      "------------------------------------------------------------\n",
      "Epoch 22 | Batch 10/297 | Loss: 0.0038 | Time: 1.6s\n",
      "Epoch 22 | Batch 20/297 | Loss: 0.0037 | Time: 3.4s\n",
      "Epoch 22 | Batch 30/297 | Loss: 0.0040 | Time: 5.3s\n",
      "Epoch 22 | Batch 40/297 | Loss: 0.0043 | Time: 7.0s\n",
      "Epoch 22 | Batch 50/297 | Loss: 0.0048 | Time: 8.8s\n",
      "Epoch 22 | Batch 60/297 | Loss: 0.0047 | Time: 10.7s\n",
      "Epoch 22 | Batch 70/297 | Loss: 0.0048 | Time: 12.5s\n",
      "Epoch 22 | Batch 80/297 | Loss: 0.0048 | Time: 14.3s\n",
      "Epoch 22 | Batch 90/297 | Loss: 0.0047 | Time: 15.9s\n",
      "Epoch 22 | Batch 100/297 | Loss: 0.0045 | Time: 17.6s\n",
      "Epoch 22 | Batch 110/297 | Loss: 0.0045 | Time: 19.5s\n",
      "Epoch 22 | Batch 120/297 | Loss: 0.0046 | Time: 21.2s\n",
      "Epoch 22 | Batch 130/297 | Loss: 0.0045 | Time: 23.2s\n",
      "Epoch 22 | Batch 140/297 | Loss: 0.0046 | Time: 25.0s\n",
      "Epoch 22 | Batch 150/297 | Loss: 0.0047 | Time: 26.7s\n",
      "Epoch 22 | Batch 160/297 | Loss: 0.0046 | Time: 28.5s\n",
      "Epoch 22 | Batch 170/297 | Loss: 0.0046 | Time: 30.4s\n",
      "Epoch 22 | Batch 180/297 | Loss: 0.0046 | Time: 32.2s\n",
      "Epoch 22 | Batch 190/297 | Loss: 0.0046 | Time: 33.9s\n",
      "Epoch 22 | Batch 200/297 | Loss: 0.0046 | Time: 35.7s\n",
      "Epoch 22 | Batch 210/297 | Loss: 0.0046 | Time: 37.5s\n",
      "Epoch 22 | Batch 220/297 | Loss: 0.0046 | Time: 39.4s\n",
      "Epoch 22 | Batch 230/297 | Loss: 0.0045 | Time: 41.0s\n",
      "Epoch 22 | Batch 240/297 | Loss: 0.0046 | Time: 42.7s\n",
      "Epoch 22 | Batch 250/297 | Loss: 0.0045 | Time: 44.5s\n",
      "Epoch 22 | Batch 260/297 | Loss: 0.0045 | Time: 46.3s\n",
      "Epoch 22 | Batch 270/297 | Loss: 0.0045 | Time: 48.0s\n",
      "Epoch 22 | Batch 280/297 | Loss: 0.0046 | Time: 49.7s\n",
      "Epoch 22 | Batch 290/297 | Loss: 0.0045 | Time: 51.3s\n",
      "Train Loss: 0.0045\n",
      "Val Loss: 0.3286\n",
      "Learning Rate: 0.000009\n",
      "\n",
      "Epoch 23/50\n",
      "------------------------------------------------------------\n",
      "Epoch 23 | Batch 10/297 | Loss: 0.0058 | Time: 1.8s\n",
      "Epoch 23 | Batch 20/297 | Loss: 0.0057 | Time: 3.6s\n",
      "Epoch 23 | Batch 30/297 | Loss: 0.0050 | Time: 5.4s\n",
      "Epoch 23 | Batch 40/297 | Loss: 0.0062 | Time: 7.1s\n",
      "Epoch 23 | Batch 50/297 | Loss: 0.0060 | Time: 9.0s\n",
      "Epoch 23 | Batch 60/297 | Loss: 0.0059 | Time: 10.8s\n",
      "Epoch 23 | Batch 70/297 | Loss: 0.0057 | Time: 12.4s\n",
      "Epoch 23 | Batch 80/297 | Loss: 0.0055 | Time: 14.3s\n",
      "Epoch 23 | Batch 90/297 | Loss: 0.0053 | Time: 16.1s\n",
      "Epoch 23 | Batch 100/297 | Loss: 0.0051 | Time: 17.8s\n",
      "Epoch 23 | Batch 110/297 | Loss: 0.0050 | Time: 19.5s\n",
      "Epoch 23 | Batch 120/297 | Loss: 0.0048 | Time: 21.2s\n",
      "Epoch 23 | Batch 130/297 | Loss: 0.0049 | Time: 23.0s\n",
      "Epoch 23 | Batch 140/297 | Loss: 0.0050 | Time: 24.8s\n",
      "Epoch 23 | Batch 150/297 | Loss: 0.0050 | Time: 26.4s\n",
      "Epoch 23 | Batch 160/297 | Loss: 0.0049 | Time: 28.2s\n",
      "Epoch 23 | Batch 170/297 | Loss: 0.0049 | Time: 29.9s\n",
      "Epoch 23 | Batch 180/297 | Loss: 0.0049 | Time: 31.6s\n",
      "Epoch 23 | Batch 190/297 | Loss: 0.0049 | Time: 33.5s\n",
      "Epoch 23 | Batch 200/297 | Loss: 0.0049 | Time: 35.4s\n",
      "Epoch 23 | Batch 210/297 | Loss: 0.0048 | Time: 37.3s\n",
      "Epoch 23 | Batch 220/297 | Loss: 0.0047 | Time: 39.1s\n",
      "Epoch 23 | Batch 230/297 | Loss: 0.0046 | Time: 40.9s\n",
      "Epoch 23 | Batch 240/297 | Loss: 0.0046 | Time: 42.6s\n",
      "Epoch 23 | Batch 250/297 | Loss: 0.0047 | Time: 44.4s\n",
      "Epoch 23 | Batch 260/297 | Loss: 0.0047 | Time: 46.2s\n",
      "Epoch 23 | Batch 270/297 | Loss: 0.0046 | Time: 47.9s\n",
      "Epoch 23 | Batch 280/297 | Loss: 0.0046 | Time: 49.6s\n",
      "Epoch 23 | Batch 290/297 | Loss: 0.0045 | Time: 51.4s\n",
      "Train Loss: 0.0045\n",
      "Val Loss: 0.3274\n",
      "Learning Rate: 0.000009\n",
      "\n",
      "Epoch 24/50\n",
      "------------------------------------------------------------\n",
      "Epoch 24 | Batch 10/297 | Loss: 0.0071 | Time: 1.8s\n",
      "Epoch 24 | Batch 20/297 | Loss: 0.0046 | Time: 3.6s\n",
      "Epoch 24 | Batch 30/297 | Loss: 0.0039 | Time: 5.4s\n",
      "Epoch 24 | Batch 40/297 | Loss: 0.0036 | Time: 7.2s\n",
      "Epoch 24 | Batch 50/297 | Loss: 0.0036 | Time: 9.2s\n",
      "Epoch 24 | Batch 60/297 | Loss: 0.0035 | Time: 11.0s\n",
      "Epoch 24 | Batch 70/297 | Loss: 0.0036 | Time: 12.6s\n",
      "Epoch 24 | Batch 80/297 | Loss: 0.0037 | Time: 14.4s\n",
      "Epoch 24 | Batch 90/297 | Loss: 0.0038 | Time: 16.1s\n",
      "Epoch 24 | Batch 100/297 | Loss: 0.0038 | Time: 17.9s\n",
      "Epoch 24 | Batch 110/297 | Loss: 0.0039 | Time: 19.8s\n",
      "Epoch 24 | Batch 120/297 | Loss: 0.0038 | Time: 21.5s\n",
      "Epoch 24 | Batch 130/297 | Loss: 0.0038 | Time: 23.3s\n",
      "Epoch 24 | Batch 140/297 | Loss: 0.0038 | Time: 25.0s\n",
      "Epoch 24 | Batch 150/297 | Loss: 0.0037 | Time: 26.9s\n",
      "Epoch 24 | Batch 160/297 | Loss: 0.0037 | Time: 28.8s\n",
      "Epoch 24 | Batch 170/297 | Loss: 0.0037 | Time: 30.6s\n",
      "Epoch 24 | Batch 180/297 | Loss: 0.0037 | Time: 32.6s\n",
      "Epoch 24 | Batch 190/297 | Loss: 0.0036 | Time: 34.5s\n",
      "Epoch 24 | Batch 200/297 | Loss: 0.0037 | Time: 36.1s\n",
      "Epoch 24 | Batch 210/297 | Loss: 0.0038 | Time: 37.7s\n",
      "Epoch 24 | Batch 220/297 | Loss: 0.0038 | Time: 39.6s\n",
      "Epoch 24 | Batch 230/297 | Loss: 0.0038 | Time: 41.3s\n",
      "Epoch 24 | Batch 240/297 | Loss: 0.0037 | Time: 42.9s\n",
      "Epoch 24 | Batch 250/297 | Loss: 0.0037 | Time: 44.7s\n",
      "Epoch 24 | Batch 260/297 | Loss: 0.0037 | Time: 46.5s\n",
      "Epoch 24 | Batch 270/297 | Loss: 0.0037 | Time: 48.4s\n",
      "Epoch 24 | Batch 280/297 | Loss: 0.0038 | Time: 50.5s\n",
      "Epoch 24 | Batch 290/297 | Loss: 0.0038 | Time: 52.2s\n",
      "Train Loss: 0.0039\n",
      "Val Loss: 0.3266\n",
      "Learning Rate: 0.000009\n",
      "\n",
      "Epoch 25/50\n",
      "------------------------------------------------------------\n",
      "Epoch 25 | Batch 10/297 | Loss: 0.0034 | Time: 1.5s\n",
      "Epoch 25 | Batch 20/297 | Loss: 0.0029 | Time: 3.3s\n",
      "Epoch 25 | Batch 30/297 | Loss: 0.0027 | Time: 5.2s\n",
      "Epoch 25 | Batch 40/297 | Loss: 0.0030 | Time: 6.9s\n",
      "Epoch 25 | Batch 50/297 | Loss: 0.0030 | Time: 8.9s\n",
      "Epoch 25 | Batch 60/297 | Loss: 0.0030 | Time: 10.6s\n",
      "Epoch 25 | Batch 70/297 | Loss: 0.0032 | Time: 12.4s\n",
      "Epoch 25 | Batch 80/297 | Loss: 0.0031 | Time: 14.2s\n",
      "Epoch 25 | Batch 90/297 | Loss: 0.0033 | Time: 16.0s\n",
      "Epoch 25 | Batch 100/297 | Loss: 0.0033 | Time: 17.5s\n",
      "Epoch 25 | Batch 110/297 | Loss: 0.0033 | Time: 19.3s\n",
      "Epoch 25 | Batch 120/297 | Loss: 0.0035 | Time: 21.2s\n",
      "Epoch 25 | Batch 130/297 | Loss: 0.0036 | Time: 22.9s\n",
      "Epoch 25 | Batch 140/297 | Loss: 0.0036 | Time: 24.7s\n",
      "Epoch 25 | Batch 150/297 | Loss: 0.0037 | Time: 26.4s\n",
      "Epoch 25 | Batch 160/297 | Loss: 0.0037 | Time: 28.1s\n",
      "Epoch 25 | Batch 170/297 | Loss: 0.0037 | Time: 30.0s\n",
      "Epoch 25 | Batch 180/297 | Loss: 0.0036 | Time: 31.9s\n",
      "Epoch 25 | Batch 190/297 | Loss: 0.0036 | Time: 34.0s\n",
      "Epoch 25 | Batch 200/297 | Loss: 0.0036 | Time: 35.8s\n",
      "Epoch 25 | Batch 210/297 | Loss: 0.0037 | Time: 37.5s\n",
      "Epoch 25 | Batch 220/297 | Loss: 0.0036 | Time: 39.2s\n",
      "Epoch 25 | Batch 230/297 | Loss: 0.0038 | Time: 41.0s\n",
      "Epoch 25 | Batch 240/297 | Loss: 0.0038 | Time: 42.9s\n",
      "Epoch 25 | Batch 250/297 | Loss: 0.0038 | Time: 44.8s\n",
      "Epoch 25 | Batch 260/297 | Loss: 0.0038 | Time: 46.6s\n",
      "Epoch 25 | Batch 270/297 | Loss: 0.0038 | Time: 48.4s\n",
      "Epoch 25 | Batch 280/297 | Loss: 0.0038 | Time: 50.0s\n",
      "Epoch 25 | Batch 290/297 | Loss: 0.0038 | Time: 51.7s\n",
      "Train Loss: 0.0038\n",
      "Val Loss: 0.3307\n",
      "Learning Rate: 0.000005\n",
      "\n",
      "Epoch 26/50\n",
      "------------------------------------------------------------\n",
      "Epoch 26 | Batch 10/297 | Loss: 0.0026 | Time: 1.7s\n",
      "Epoch 26 | Batch 20/297 | Loss: 0.0033 | Time: 3.4s\n",
      "Epoch 26 | Batch 30/297 | Loss: 0.0031 | Time: 5.1s\n",
      "Epoch 26 | Batch 40/297 | Loss: 0.0032 | Time: 7.0s\n",
      "Epoch 26 | Batch 50/297 | Loss: 0.0034 | Time: 8.8s\n",
      "Epoch 26 | Batch 60/297 | Loss: 0.0034 | Time: 10.5s\n",
      "Epoch 26 | Batch 70/297 | Loss: 0.0044 | Time: 12.5s\n",
      "Epoch 26 | Batch 80/297 | Loss: 0.0042 | Time: 14.2s\n",
      "Epoch 26 | Batch 90/297 | Loss: 0.0040 | Time: 16.0s\n",
      "Epoch 26 | Batch 100/297 | Loss: 0.0038 | Time: 17.6s\n",
      "Epoch 26 | Batch 110/297 | Loss: 0.0039 | Time: 19.4s\n",
      "Epoch 26 | Batch 120/297 | Loss: 0.0038 | Time: 21.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Batch 130/297 | Loss: 0.0039 | Time: 23.1s\n",
      "Epoch 26 | Batch 140/297 | Loss: 0.0038 | Time: 24.9s\n",
      "Epoch 26 | Batch 150/297 | Loss: 0.0038 | Time: 26.8s\n",
      "Epoch 26 | Batch 160/297 | Loss: 0.0038 | Time: 28.6s\n",
      "Epoch 26 | Batch 170/297 | Loss: 0.0038 | Time: 30.2s\n",
      "Epoch 26 | Batch 180/297 | Loss: 0.0037 | Time: 31.9s\n",
      "Epoch 26 | Batch 190/297 | Loss: 0.0037 | Time: 33.6s\n",
      "Epoch 26 | Batch 200/297 | Loss: 0.0037 | Time: 35.2s\n",
      "Epoch 26 | Batch 210/297 | Loss: 0.0037 | Time: 37.0s\n",
      "Epoch 26 | Batch 220/297 | Loss: 0.0037 | Time: 38.8s\n",
      "Epoch 26 | Batch 230/297 | Loss: 0.0038 | Time: 40.5s\n",
      "Epoch 26 | Batch 240/297 | Loss: 0.0038 | Time: 42.3s\n",
      "Epoch 26 | Batch 250/297 | Loss: 0.0038 | Time: 44.1s\n",
      "Epoch 26 | Batch 260/297 | Loss: 0.0040 | Time: 46.1s\n",
      "Epoch 26 | Batch 270/297 | Loss: 0.0040 | Time: 47.9s\n",
      "Epoch 26 | Batch 280/297 | Loss: 0.0040 | Time: 49.7s\n",
      "Epoch 26 | Batch 290/297 | Loss: 0.0040 | Time: 51.6s\n",
      "Train Loss: 0.0041\n",
      "Val Loss: 0.3268\n",
      "Learning Rate: 0.000005\n",
      "\n",
      "Epoch 27/50\n",
      "------------------------------------------------------------\n",
      "Epoch 27 | Batch 10/297 | Loss: 0.0037 | Time: 1.6s\n",
      "Epoch 27 | Batch 20/297 | Loss: 0.0031 | Time: 3.5s\n",
      "Epoch 27 | Batch 30/297 | Loss: 0.0032 | Time: 5.3s\n",
      "Epoch 27 | Batch 40/297 | Loss: 0.0035 | Time: 7.2s\n",
      "Epoch 27 | Batch 50/297 | Loss: 0.0036 | Time: 8.9s\n",
      "Epoch 27 | Batch 60/297 | Loss: 0.0038 | Time: 10.8s\n",
      "Epoch 27 | Batch 70/297 | Loss: 0.0036 | Time: 12.5s\n",
      "Epoch 27 | Batch 80/297 | Loss: 0.0035 | Time: 14.4s\n",
      "Epoch 27 | Batch 90/297 | Loss: 0.0035 | Time: 16.1s\n",
      "Epoch 27 | Batch 100/297 | Loss: 0.0034 | Time: 17.6s\n",
      "Epoch 27 | Batch 110/297 | Loss: 0.0033 | Time: 19.2s\n",
      "Epoch 27 | Batch 120/297 | Loss: 0.0032 | Time: 20.9s\n",
      "Epoch 27 | Batch 130/297 | Loss: 0.0034 | Time: 22.6s\n",
      "Epoch 27 | Batch 140/297 | Loss: 0.0034 | Time: 24.4s\n",
      "Epoch 27 | Batch 150/297 | Loss: 0.0033 | Time: 26.1s\n",
      "Epoch 27 | Batch 160/297 | Loss: 0.0034 | Time: 28.0s\n",
      "Epoch 27 | Batch 170/297 | Loss: 0.0034 | Time: 29.9s\n",
      "Epoch 27 | Batch 180/297 | Loss: 0.0034 | Time: 31.6s\n",
      "Epoch 27 | Batch 190/297 | Loss: 0.0035 | Time: 33.4s\n",
      "Epoch 27 | Batch 200/297 | Loss: 0.0035 | Time: 35.3s\n",
      "Epoch 27 | Batch 210/297 | Loss: 0.0035 | Time: 37.1s\n",
      "Epoch 27 | Batch 220/297 | Loss: 0.0034 | Time: 39.0s\n",
      "Epoch 27 | Batch 230/297 | Loss: 0.0034 | Time: 40.7s\n",
      "Epoch 27 | Batch 240/297 | Loss: 0.0034 | Time: 42.3s\n",
      "Epoch 27 | Batch 250/297 | Loss: 0.0033 | Time: 44.2s\n",
      "Epoch 27 | Batch 260/297 | Loss: 0.0034 | Time: 45.9s\n",
      "Epoch 27 | Batch 270/297 | Loss: 0.0034 | Time: 47.7s\n",
      "Epoch 27 | Batch 280/297 | Loss: 0.0033 | Time: 49.6s\n",
      "Epoch 27 | Batch 290/297 | Loss: 0.0033 | Time: 51.2s\n",
      "Train Loss: 0.0033\n",
      "Val Loss: 0.3258\n",
      "Learning Rate: 0.000005\n",
      "\n",
      "Epoch 28/50\n",
      "------------------------------------------------------------\n",
      "Epoch 28 | Batch 10/297 | Loss: 0.0036 | Time: 1.8s\n",
      "Epoch 28 | Batch 20/297 | Loss: 0.0055 | Time: 3.7s\n",
      "Epoch 28 | Batch 30/297 | Loss: 0.0048 | Time: 5.4s\n",
      "Epoch 28 | Batch 40/297 | Loss: 0.0048 | Time: 7.5s\n",
      "Epoch 28 | Batch 50/297 | Loss: 0.0043 | Time: 9.3s\n",
      "Epoch 28 | Batch 60/297 | Loss: 0.0042 | Time: 11.1s\n",
      "Epoch 28 | Batch 70/297 | Loss: 0.0043 | Time: 12.9s\n",
      "Epoch 28 | Batch 80/297 | Loss: 0.0042 | Time: 14.6s\n",
      "Epoch 28 | Batch 90/297 | Loss: 0.0041 | Time: 16.2s\n",
      "Epoch 28 | Batch 100/297 | Loss: 0.0039 | Time: 17.9s\n",
      "Epoch 28 | Batch 110/297 | Loss: 0.0039 | Time: 19.7s\n",
      "Epoch 28 | Batch 120/297 | Loss: 0.0040 | Time: 21.4s\n",
      "Epoch 28 | Batch 130/297 | Loss: 0.0040 | Time: 23.0s\n",
      "Epoch 28 | Batch 140/297 | Loss: 0.0040 | Time: 24.6s\n",
      "Epoch 28 | Batch 150/297 | Loss: 0.0040 | Time: 26.4s\n",
      "Epoch 28 | Batch 160/297 | Loss: 0.0040 | Time: 28.2s\n",
      "Epoch 28 | Batch 170/297 | Loss: 0.0041 | Time: 30.1s\n",
      "Epoch 28 | Batch 180/297 | Loss: 0.0040 | Time: 31.8s\n",
      "Epoch 28 | Batch 190/297 | Loss: 0.0041 | Time: 33.7s\n",
      "Epoch 28 | Batch 200/297 | Loss: 0.0040 | Time: 35.4s\n",
      "Epoch 28 | Batch 210/297 | Loss: 0.0040 | Time: 37.3s\n",
      "Epoch 28 | Batch 220/297 | Loss: 0.0039 | Time: 39.2s\n",
      "Epoch 28 | Batch 230/297 | Loss: 0.0040 | Time: 40.8s\n",
      "Epoch 28 | Batch 240/297 | Loss: 0.0040 | Time: 42.6s\n",
      "Epoch 28 | Batch 250/297 | Loss: 0.0040 | Time: 44.4s\n",
      "Epoch 28 | Batch 260/297 | Loss: 0.0041 | Time: 46.2s\n",
      "Epoch 28 | Batch 270/297 | Loss: 0.0040 | Time: 48.1s\n",
      "Epoch 28 | Batch 280/297 | Loss: 0.0040 | Time: 50.0s\n",
      "Epoch 28 | Batch 290/297 | Loss: 0.0039 | Time: 51.8s\n",
      "Train Loss: 0.0039\n",
      "Val Loss: 0.3281\n",
      "Learning Rate: 0.000002\n",
      "\n",
      "Epoch 29/50\n",
      "------------------------------------------------------------\n",
      "Epoch 29 | Batch 10/297 | Loss: 0.0042 | Time: 1.7s\n",
      "Epoch 29 | Batch 20/297 | Loss: 0.0032 | Time: 3.6s\n",
      "Epoch 29 | Batch 30/297 | Loss: 0.0034 | Time: 5.4s\n",
      "Epoch 29 | Batch 40/297 | Loss: 0.0031 | Time: 7.0s\n",
      "Epoch 29 | Batch 50/297 | Loss: 0.0029 | Time: 8.7s\n",
      "Epoch 29 | Batch 60/297 | Loss: 0.0030 | Time: 10.6s\n",
      "Epoch 29 | Batch 70/297 | Loss: 0.0034 | Time: 12.6s\n",
      "Epoch 29 | Batch 80/297 | Loss: 0.0033 | Time: 14.5s\n",
      "Epoch 29 | Batch 90/297 | Loss: 0.0037 | Time: 16.3s\n",
      "Epoch 29 | Batch 100/297 | Loss: 0.0038 | Time: 18.0s\n",
      "Epoch 29 | Batch 110/297 | Loss: 0.0037 | Time: 19.7s\n",
      "Epoch 29 | Batch 120/297 | Loss: 0.0036 | Time: 21.4s\n",
      "Epoch 29 | Batch 130/297 | Loss: 0.0037 | Time: 23.2s\n",
      "Epoch 29 | Batch 140/297 | Loss: 0.0037 | Time: 25.0s\n",
      "Epoch 29 | Batch 150/297 | Loss: 0.0036 | Time: 26.9s\n",
      "Epoch 29 | Batch 160/297 | Loss: 0.0035 | Time: 28.7s\n",
      "Epoch 29 | Batch 170/297 | Loss: 0.0035 | Time: 30.3s\n",
      "Epoch 29 | Batch 180/297 | Loss: 0.0034 | Time: 32.0s\n",
      "Epoch 29 | Batch 190/297 | Loss: 0.0034 | Time: 33.9s\n",
      "Epoch 29 | Batch 200/297 | Loss: 0.0034 | Time: 35.7s\n",
      "Epoch 29 | Batch 210/297 | Loss: 0.0034 | Time: 37.5s\n",
      "Epoch 29 | Batch 220/297 | Loss: 0.0035 | Time: 39.3s\n",
      "Epoch 29 | Batch 230/297 | Loss: 0.0035 | Time: 41.1s\n",
      "Epoch 29 | Batch 240/297 | Loss: 0.0035 | Time: 42.7s\n",
      "Epoch 29 | Batch 250/297 | Loss: 0.0035 | Time: 44.4s\n",
      "Epoch 29 | Batch 260/297 | Loss: 0.0035 | Time: 46.2s\n",
      "Epoch 29 | Batch 270/297 | Loss: 0.0035 | Time: 47.8s\n",
      "Epoch 29 | Batch 280/297 | Loss: 0.0035 | Time: 49.5s\n",
      "Epoch 29 | Batch 290/297 | Loss: 0.0036 | Time: 51.3s\n",
      "Train Loss: 0.0035\n",
      "Val Loss: 0.3281\n",
      "Learning Rate: 0.000002\n",
      "\n",
      "Epoch 30/50\n",
      "------------------------------------------------------------\n",
      "Epoch 30 | Batch 10/297 | Loss: 0.0030 | Time: 1.7s\n",
      "Epoch 30 | Batch 20/297 | Loss: 0.0025 | Time: 3.5s\n",
      "Epoch 30 | Batch 30/297 | Loss: 0.0027 | Time: 5.0s\n",
      "Epoch 30 | Batch 40/297 | Loss: 0.0025 | Time: 6.8s\n",
      "Epoch 30 | Batch 50/297 | Loss: 0.0030 | Time: 8.5s\n",
      "Epoch 30 | Batch 60/297 | Loss: 0.0030 | Time: 10.2s\n",
      "Epoch 30 | Batch 70/297 | Loss: 0.0029 | Time: 12.0s\n",
      "Epoch 30 | Batch 80/297 | Loss: 0.0029 | Time: 13.8s\n",
      "Epoch 30 | Batch 90/297 | Loss: 0.0028 | Time: 15.6s\n",
      "Epoch 30 | Batch 100/297 | Loss: 0.0028 | Time: 17.5s\n",
      "Epoch 30 | Batch 110/297 | Loss: 0.0029 | Time: 19.2s\n",
      "Epoch 30 | Batch 120/297 | Loss: 0.0029 | Time: 20.9s\n",
      "Epoch 30 | Batch 130/297 | Loss: 0.0029 | Time: 22.6s\n",
      "Epoch 30 | Batch 140/297 | Loss: 0.0030 | Time: 24.1s\n",
      "Epoch 30 | Batch 150/297 | Loss: 0.0030 | Time: 25.8s\n",
      "Epoch 30 | Batch 160/297 | Loss: 0.0030 | Time: 27.6s\n",
      "Epoch 30 | Batch 170/297 | Loss: 0.0031 | Time: 29.4s\n",
      "Epoch 30 | Batch 180/297 | Loss: 0.0032 | Time: 31.1s\n",
      "Epoch 30 | Batch 190/297 | Loss: 0.0033 | Time: 32.8s\n",
      "Epoch 30 | Batch 200/297 | Loss: 0.0033 | Time: 34.5s\n",
      "Epoch 30 | Batch 210/297 | Loss: 0.0033 | Time: 36.2s\n",
      "Epoch 30 | Batch 220/297 | Loss: 0.0033 | Time: 38.1s\n",
      "Epoch 30 | Batch 230/297 | Loss: 0.0032 | Time: 40.0s\n",
      "Epoch 30 | Batch 240/297 | Loss: 0.0033 | Time: 42.0s\n",
      "Epoch 30 | Batch 250/297 | Loss: 0.0034 | Time: 43.7s\n",
      "Epoch 30 | Batch 260/297 | Loss: 0.0034 | Time: 45.7s\n",
      "Epoch 30 | Batch 270/297 | Loss: 0.0034 | Time: 47.5s\n",
      "Epoch 30 | Batch 280/297 | Loss: 0.0034 | Time: 49.4s\n",
      "Epoch 30 | Batch 290/297 | Loss: 0.0034 | Time: 51.1s\n",
      "Train Loss: 0.0034\n",
      "Val Loss: 0.3271\n",
      "Learning Rate: 0.000002\n",
      "\n",
      "Epoch 31/50\n",
      "------------------------------------------------------------\n",
      "Epoch 31 | Batch 10/297 | Loss: 0.0034 | Time: 1.6s\n",
      "Epoch 31 | Batch 20/297 | Loss: 0.0028 | Time: 3.3s\n",
      "Epoch 31 | Batch 30/297 | Loss: 0.0032 | Time: 5.0s\n",
      "Epoch 31 | Batch 40/297 | Loss: 0.0029 | Time: 6.8s\n",
      "Epoch 31 | Batch 50/297 | Loss: 0.0030 | Time: 8.7s\n",
      "Epoch 31 | Batch 60/297 | Loss: 0.0033 | Time: 10.4s\n",
      "Epoch 31 | Batch 70/297 | Loss: 0.0036 | Time: 12.3s\n",
      "Epoch 31 | Batch 80/297 | Loss: 0.0036 | Time: 14.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 | Batch 90/297 | Loss: 0.0037 | Time: 15.9s\n",
      "Epoch 31 | Batch 100/297 | Loss: 0.0035 | Time: 17.8s\n",
      "Epoch 31 | Batch 110/297 | Loss: 0.0034 | Time: 19.6s\n",
      "Epoch 31 | Batch 120/297 | Loss: 0.0034 | Time: 21.3s\n",
      "Epoch 31 | Batch 130/297 | Loss: 0.0034 | Time: 23.1s\n",
      "Epoch 31 | Batch 140/297 | Loss: 0.0034 | Time: 24.8s\n",
      "Epoch 31 | Batch 150/297 | Loss: 0.0033 | Time: 26.5s\n",
      "Epoch 31 | Batch 160/297 | Loss: 0.0034 | Time: 28.3s\n",
      "Epoch 31 | Batch 170/297 | Loss: 0.0034 | Time: 30.2s\n",
      "Epoch 31 | Batch 180/297 | Loss: 0.0034 | Time: 31.7s\n",
      "Epoch 31 | Batch 190/297 | Loss: 0.0034 | Time: 33.6s\n",
      "Epoch 31 | Batch 200/297 | Loss: 0.0034 | Time: 35.2s\n",
      "Epoch 31 | Batch 210/297 | Loss: 0.0033 | Time: 37.1s\n",
      "Epoch 31 | Batch 220/297 | Loss: 0.0033 | Time: 38.8s\n",
      "Epoch 31 | Batch 230/297 | Loss: 0.0033 | Time: 40.7s\n",
      "Epoch 31 | Batch 240/297 | Loss: 0.0033 | Time: 42.6s\n",
      "Epoch 31 | Batch 250/297 | Loss: 0.0034 | Time: 44.5s\n",
      "Epoch 31 | Batch 260/297 | Loss: 0.0033 | Time: 46.4s\n",
      "Epoch 31 | Batch 270/297 | Loss: 0.0034 | Time: 48.1s\n",
      "Epoch 31 | Batch 280/297 | Loss: 0.0034 | Time: 50.0s\n",
      "Epoch 31 | Batch 290/297 | Loss: 0.0033 | Time: 51.7s\n",
      "Train Loss: 0.0033\n",
      "Val Loss: 0.3267\n",
      "Learning Rate: 0.000001\n",
      "\n",
      "Epoch 32/50\n",
      "------------------------------------------------------------\n",
      "Epoch 32 | Batch 10/297 | Loss: 0.0026 | Time: 1.7s\n",
      "Epoch 32 | Batch 20/297 | Loss: 0.0031 | Time: 3.4s\n",
      "Epoch 32 | Batch 30/297 | Loss: 0.0033 | Time: 5.3s\n",
      "Epoch 32 | Batch 40/297 | Loss: 0.0033 | Time: 7.2s\n",
      "Epoch 32 | Batch 50/297 | Loss: 0.0035 | Time: 8.8s\n",
      "Epoch 32 | Batch 60/297 | Loss: 0.0035 | Time: 10.6s\n",
      "Epoch 32 | Batch 70/297 | Loss: 0.0033 | Time: 12.4s\n",
      "Epoch 32 | Batch 80/297 | Loss: 0.0033 | Time: 14.2s\n",
      "Epoch 32 | Batch 90/297 | Loss: 0.0033 | Time: 16.1s\n",
      "Epoch 32 | Batch 100/297 | Loss: 0.0033 | Time: 18.0s\n",
      "Epoch 32 | Batch 110/297 | Loss: 0.0033 | Time: 19.8s\n",
      "Epoch 32 | Batch 120/297 | Loss: 0.0032 | Time: 21.4s\n",
      "Epoch 32 | Batch 130/297 | Loss: 0.0032 | Time: 23.3s\n",
      "Epoch 32 | Batch 140/297 | Loss: 0.0032 | Time: 25.0s\n",
      "Epoch 32 | Batch 150/297 | Loss: 0.0031 | Time: 26.9s\n",
      "Epoch 32 | Batch 160/297 | Loss: 0.0031 | Time: 28.8s\n",
      "Epoch 32 | Batch 170/297 | Loss: 0.0031 | Time: 30.5s\n",
      "Epoch 32 | Batch 180/297 | Loss: 0.0031 | Time: 32.3s\n",
      "Epoch 32 | Batch 190/297 | Loss: 0.0030 | Time: 34.1s\n",
      "Epoch 32 | Batch 200/297 | Loss: 0.0030 | Time: 35.7s\n",
      "Epoch 32 | Batch 210/297 | Loss: 0.0031 | Time: 37.5s\n",
      "Epoch 32 | Batch 220/297 | Loss: 0.0031 | Time: 39.2s\n",
      "Epoch 32 | Batch 230/297 | Loss: 0.0031 | Time: 40.8s\n",
      "Epoch 32 | Batch 240/297 | Loss: 0.0030 | Time: 42.6s\n",
      "Epoch 32 | Batch 250/297 | Loss: 0.0031 | Time: 44.4s\n",
      "Epoch 32 | Batch 260/297 | Loss: 0.0032 | Time: 46.5s\n",
      "Epoch 32 | Batch 270/297 | Loss: 0.0031 | Time: 48.2s\n",
      "Epoch 32 | Batch 280/297 | Loss: 0.0031 | Time: 49.9s\n",
      "Epoch 32 | Batch 290/297 | Loss: 0.0031 | Time: 51.6s\n",
      "Train Loss: 0.0031\n",
      "Val Loss: 0.3266\n",
      "Learning Rate: 0.000001\n",
      "\n",
      "Epoch 33/50\n",
      "------------------------------------------------------------\n",
      "Epoch 33 | Batch 10/297 | Loss: 0.0029 | Time: 1.7s\n",
      "Epoch 33 | Batch 20/297 | Loss: 0.0029 | Time: 3.5s\n",
      "Epoch 33 | Batch 30/297 | Loss: 0.0029 | Time: 5.2s\n",
      "Epoch 33 | Batch 40/297 | Loss: 0.0029 | Time: 7.0s\n",
      "Epoch 33 | Batch 50/297 | Loss: 0.0028 | Time: 8.7s\n",
      "Epoch 33 | Batch 60/297 | Loss: 0.0028 | Time: 10.5s\n",
      "Epoch 33 | Batch 70/297 | Loss: 0.0028 | Time: 12.3s\n",
      "Epoch 33 | Batch 80/297 | Loss: 0.0030 | Time: 14.0s\n",
      "Epoch 33 | Batch 90/297 | Loss: 0.0031 | Time: 15.7s\n",
      "Epoch 33 | Batch 100/297 | Loss: 0.0032 | Time: 17.5s\n",
      "Epoch 33 | Batch 110/297 | Loss: 0.0034 | Time: 19.0s\n",
      "Epoch 33 | Batch 120/297 | Loss: 0.0034 | Time: 20.5s\n",
      "Epoch 33 | Batch 130/297 | Loss: 0.0033 | Time: 22.3s\n",
      "Epoch 33 | Batch 140/297 | Loss: 0.0032 | Time: 24.2s\n",
      "Epoch 33 | Batch 150/297 | Loss: 0.0032 | Time: 26.0s\n",
      "Epoch 33 | Batch 160/297 | Loss: 0.0032 | Time: 27.8s\n",
      "Epoch 33 | Batch 170/297 | Loss: 0.0033 | Time: 29.8s\n",
      "Epoch 33 | Batch 180/297 | Loss: 0.0033 | Time: 31.4s\n",
      "Epoch 33 | Batch 190/297 | Loss: 0.0035 | Time: 33.3s\n",
      "Epoch 33 | Batch 200/297 | Loss: 0.0034 | Time: 35.3s\n",
      "Epoch 33 | Batch 210/297 | Loss: 0.0034 | Time: 37.0s\n",
      "Epoch 33 | Batch 220/297 | Loss: 0.0035 | Time: 38.6s\n",
      "Epoch 33 | Batch 230/297 | Loss: 0.0035 | Time: 40.5s\n",
      "Epoch 33 | Batch 240/297 | Loss: 0.0035 | Time: 42.5s\n",
      "Epoch 33 | Batch 250/297 | Loss: 0.0035 | Time: 44.2s\n",
      "Epoch 33 | Batch 260/297 | Loss: 0.0035 | Time: 46.0s\n",
      "Epoch 33 | Batch 270/297 | Loss: 0.0037 | Time: 47.8s\n",
      "Epoch 33 | Batch 280/297 | Loss: 0.0037 | Time: 49.6s\n",
      "Epoch 33 | Batch 290/297 | Loss: 0.0037 | Time: 51.4s\n",
      "Train Loss: 0.0037\n",
      "Val Loss: 0.3258\n",
      "Learning Rate: 0.000001\n",
      "\n",
      "Epoch 34/50\n",
      "------------------------------------------------------------\n",
      "Epoch 34 | Batch 10/297 | Loss: 0.0046 | Time: 1.6s\n",
      "Epoch 34 | Batch 20/297 | Loss: 0.0037 | Time: 3.3s\n",
      "Epoch 34 | Batch 30/297 | Loss: 0.0032 | Time: 5.2s\n",
      "Epoch 34 | Batch 40/297 | Loss: 0.0031 | Time: 7.1s\n",
      "Epoch 34 | Batch 50/297 | Loss: 0.0029 | Time: 9.1s\n",
      "Epoch 34 | Batch 60/297 | Loss: 0.0034 | Time: 10.8s\n",
      "Epoch 34 | Batch 70/297 | Loss: 0.0033 | Time: 12.7s\n",
      "Epoch 34 | Batch 80/297 | Loss: 0.0037 | Time: 14.5s\n",
      "Epoch 34 | Batch 90/297 | Loss: 0.0037 | Time: 16.3s\n",
      "Epoch 34 | Batch 100/297 | Loss: 0.0036 | Time: 18.0s\n",
      "Epoch 34 | Batch 110/297 | Loss: 0.0035 | Time: 19.9s\n",
      "Epoch 34 | Batch 120/297 | Loss: 0.0036 | Time: 21.8s\n",
      "Epoch 34 | Batch 130/297 | Loss: 0.0035 | Time: 23.7s\n",
      "Epoch 34 | Batch 140/297 | Loss: 0.0036 | Time: 25.5s\n",
      "Epoch 34 | Batch 150/297 | Loss: 0.0035 | Time: 27.4s\n",
      "Epoch 34 | Batch 160/297 | Loss: 0.0035 | Time: 29.2s\n",
      "Epoch 34 | Batch 170/297 | Loss: 0.0035 | Time: 30.9s\n",
      "Epoch 34 | Batch 180/297 | Loss: 0.0034 | Time: 32.7s\n",
      "Epoch 34 | Batch 190/297 | Loss: 0.0034 | Time: 34.5s\n",
      "Epoch 34 | Batch 200/297 | Loss: 0.0035 | Time: 36.2s\n",
      "Epoch 34 | Batch 210/297 | Loss: 0.0035 | Time: 37.8s\n",
      "Epoch 34 | Batch 220/297 | Loss: 0.0035 | Time: 39.7s\n",
      "Epoch 34 | Batch 230/297 | Loss: 0.0035 | Time: 41.3s\n",
      "Epoch 34 | Batch 240/297 | Loss: 0.0035 | Time: 43.0s\n",
      "Epoch 34 | Batch 250/297 | Loss: 0.0035 | Time: 44.7s\n",
      "Epoch 34 | Batch 260/297 | Loss: 0.0035 | Time: 46.7s\n",
      "Epoch 34 | Batch 270/297 | Loss: 0.0034 | Time: 48.5s\n",
      "Epoch 34 | Batch 280/297 | Loss: 0.0034 | Time: 50.2s\n",
      "Epoch 34 | Batch 290/297 | Loss: 0.0033 | Time: 51.9s\n",
      "Train Loss: 0.0033\n",
      "Val Loss: 0.3252\n",
      "Learning Rate: 0.000001\n",
      "\n",
      "Epoch 35/50\n",
      "------------------------------------------------------------\n",
      "Epoch 35 | Batch 10/297 | Loss: 0.0039 | Time: 1.8s\n",
      "Epoch 35 | Batch 20/297 | Loss: 0.0038 | Time: 3.4s\n",
      "Epoch 35 | Batch 30/297 | Loss: 0.0034 | Time: 5.1s\n",
      "Epoch 35 | Batch 40/297 | Loss: 0.0032 | Time: 7.1s\n",
      "Epoch 35 | Batch 50/297 | Loss: 0.0031 | Time: 9.1s\n",
      "Epoch 35 | Batch 60/297 | Loss: 0.0029 | Time: 10.7s\n",
      "Epoch 35 | Batch 70/297 | Loss: 0.0031 | Time: 12.6s\n",
      "Epoch 35 | Batch 80/297 | Loss: 0.0033 | Time: 14.7s\n",
      "Epoch 35 | Batch 90/297 | Loss: 0.0033 | Time: 16.4s\n",
      "Epoch 35 | Batch 100/297 | Loss: 0.0034 | Time: 18.1s\n",
      "Epoch 35 | Batch 110/297 | Loss: 0.0035 | Time: 19.8s\n",
      "Epoch 35 | Batch 120/297 | Loss: 0.0034 | Time: 21.4s\n",
      "Epoch 35 | Batch 130/297 | Loss: 0.0034 | Time: 23.0s\n",
      "Epoch 35 | Batch 140/297 | Loss: 0.0034 | Time: 24.7s\n",
      "Epoch 35 | Batch 150/297 | Loss: 0.0033 | Time: 26.6s\n",
      "Epoch 35 | Batch 160/297 | Loss: 0.0033 | Time: 28.4s\n",
      "Epoch 35 | Batch 170/297 | Loss: 0.0032 | Time: 30.3s\n",
      "Epoch 35 | Batch 180/297 | Loss: 0.0034 | Time: 31.9s\n",
      "Epoch 35 | Batch 190/297 | Loss: 0.0034 | Time: 33.7s\n",
      "Epoch 35 | Batch 200/297 | Loss: 0.0034 | Time: 35.5s\n",
      "Epoch 35 | Batch 210/297 | Loss: 0.0034 | Time: 37.3s\n",
      "Epoch 35 | Batch 220/297 | Loss: 0.0034 | Time: 39.1s\n",
      "Epoch 35 | Batch 230/297 | Loss: 0.0034 | Time: 40.8s\n",
      "Epoch 35 | Batch 240/297 | Loss: 0.0034 | Time: 42.7s\n",
      "Epoch 35 | Batch 250/297 | Loss: 0.0034 | Time: 44.5s\n",
      "Epoch 35 | Batch 260/297 | Loss: 0.0034 | Time: 46.3s\n",
      "Epoch 35 | Batch 270/297 | Loss: 0.0034 | Time: 48.2s\n",
      "Epoch 35 | Batch 280/297 | Loss: 0.0034 | Time: 50.1s\n",
      "Epoch 35 | Batch 290/297 | Loss: 0.0034 | Time: 51.9s\n",
      "Train Loss: 0.0034\n",
      "Val Loss: 0.3252\n",
      "Learning Rate: 0.000001\n",
      "\n",
      "Epoch 36/50\n",
      "------------------------------------------------------------\n",
      "Epoch 36 | Batch 10/297 | Loss: 0.0028 | Time: 1.7s\n",
      "Epoch 36 | Batch 20/297 | Loss: 0.0045 | Time: 3.2s\n",
      "Epoch 36 | Batch 30/297 | Loss: 0.0038 | Time: 5.1s\n",
      "Epoch 36 | Batch 40/297 | Loss: 0.0039 | Time: 6.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 | Batch 50/297 | Loss: 0.0043 | Time: 8.8s\n",
      "Epoch 36 | Batch 60/297 | Loss: 0.0045 | Time: 10.4s\n",
      "Epoch 36 | Batch 70/297 | Loss: 0.0043 | Time: 12.5s\n",
      "Epoch 36 | Batch 80/297 | Loss: 0.0043 | Time: 14.3s\n",
      "Epoch 36 | Batch 90/297 | Loss: 0.0041 | Time: 16.1s\n",
      "Epoch 36 | Batch 100/297 | Loss: 0.0041 | Time: 17.9s\n",
      "Epoch 36 | Batch 110/297 | Loss: 0.0040 | Time: 19.6s\n",
      "Epoch 36 | Batch 120/297 | Loss: 0.0039 | Time: 21.2s\n",
      "Epoch 36 | Batch 130/297 | Loss: 0.0039 | Time: 23.2s\n",
      "Epoch 36 | Batch 140/297 | Loss: 0.0042 | Time: 25.1s\n",
      "Epoch 36 | Batch 150/297 | Loss: 0.0041 | Time: 27.0s\n",
      "Epoch 36 | Batch 160/297 | Loss: 0.0040 | Time: 28.8s\n",
      "Epoch 36 | Batch 170/297 | Loss: 0.0040 | Time: 30.4s\n",
      "Epoch 36 | Batch 180/297 | Loss: 0.0039 | Time: 32.1s\n",
      "Epoch 36 | Batch 190/297 | Loss: 0.0040 | Time: 33.7s\n",
      "Epoch 36 | Batch 200/297 | Loss: 0.0039 | Time: 35.6s\n",
      "Epoch 36 | Batch 210/297 | Loss: 0.0038 | Time: 37.2s\n",
      "Epoch 36 | Batch 220/297 | Loss: 0.0038 | Time: 39.0s\n",
      "Epoch 36 | Batch 230/297 | Loss: 0.0038 | Time: 40.9s\n",
      "Epoch 36 | Batch 240/297 | Loss: 0.0038 | Time: 42.6s\n",
      "Epoch 36 | Batch 250/297 | Loss: 0.0039 | Time: 44.5s\n",
      "Epoch 36 | Batch 260/297 | Loss: 0.0038 | Time: 46.1s\n",
      "Epoch 36 | Batch 270/297 | Loss: 0.0038 | Time: 48.0s\n",
      "Epoch 36 | Batch 280/297 | Loss: 0.0038 | Time: 50.0s\n",
      "Epoch 36 | Batch 290/297 | Loss: 0.0037 | Time: 51.6s\n",
      "Train Loss: 0.0037\n",
      "Val Loss: 0.3255\n",
      "Learning Rate: 0.000001\n",
      "\n",
      "Epoch 37/50\n",
      "------------------------------------------------------------\n",
      "Epoch 37 | Batch 10/297 | Loss: 0.0026 | Time: 1.9s\n",
      "Epoch 37 | Batch 20/297 | Loss: 0.0027 | Time: 3.8s\n",
      "Epoch 37 | Batch 30/297 | Loss: 0.0025 | Time: 5.6s\n",
      "Epoch 37 | Batch 40/297 | Loss: 0.0027 | Time: 7.4s\n",
      "Epoch 37 | Batch 50/297 | Loss: 0.0028 | Time: 9.2s\n",
      "Epoch 37 | Batch 60/297 | Loss: 0.0027 | Time: 11.0s\n",
      "Epoch 37 | Batch 70/297 | Loss: 0.0028 | Time: 12.8s\n",
      "Epoch 37 | Batch 80/297 | Loss: 0.0031 | Time: 14.6s\n",
      "Epoch 37 | Batch 90/297 | Loss: 0.0031 | Time: 16.4s\n",
      "Epoch 37 | Batch 100/297 | Loss: 0.0034 | Time: 18.1s\n",
      "Epoch 37 | Batch 110/297 | Loss: 0.0034 | Time: 19.7s\n",
      "Epoch 37 | Batch 120/297 | Loss: 0.0033 | Time: 21.7s\n",
      "Epoch 37 | Batch 130/297 | Loss: 0.0034 | Time: 23.4s\n",
      "Epoch 37 | Batch 140/297 | Loss: 0.0034 | Time: 25.3s\n",
      "Epoch 37 | Batch 150/297 | Loss: 0.0034 | Time: 27.0s\n",
      "Epoch 37 | Batch 160/297 | Loss: 0.0035 | Time: 28.8s\n",
      "Epoch 37 | Batch 170/297 | Loss: 0.0034 | Time: 30.7s\n",
      "Epoch 37 | Batch 180/297 | Loss: 0.0035 | Time: 32.6s\n",
      "Epoch 37 | Batch 190/297 | Loss: 0.0035 | Time: 34.2s\n",
      "Epoch 37 | Batch 200/297 | Loss: 0.0035 | Time: 36.0s\n",
      "Epoch 37 | Batch 210/297 | Loss: 0.0035 | Time: 37.8s\n",
      "Epoch 37 | Batch 220/297 | Loss: 0.0035 | Time: 39.6s\n",
      "Epoch 37 | Batch 230/297 | Loss: 0.0034 | Time: 41.1s\n",
      "Epoch 37 | Batch 240/297 | Loss: 0.0034 | Time: 43.1s\n",
      "Epoch 37 | Batch 250/297 | Loss: 0.0034 | Time: 44.9s\n",
      "Epoch 37 | Batch 260/297 | Loss: 0.0034 | Time: 46.8s\n",
      "Epoch 37 | Batch 270/297 | Loss: 0.0034 | Time: 48.5s\n",
      "Epoch 37 | Batch 280/297 | Loss: 0.0034 | Time: 50.2s\n",
      "Epoch 37 | Batch 290/297 | Loss: 0.0034 | Time: 52.0s\n",
      "Train Loss: 0.0034\n",
      "Val Loss: 0.3258\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 38/50\n",
      "------------------------------------------------------------\n",
      "Epoch 38 | Batch 10/297 | Loss: 0.0050 | Time: 1.7s\n",
      "Epoch 38 | Batch 20/297 | Loss: 0.0047 | Time: 3.5s\n",
      "Epoch 38 | Batch 30/297 | Loss: 0.0039 | Time: 5.3s\n",
      "Epoch 38 | Batch 40/297 | Loss: 0.0036 | Time: 6.9s\n",
      "Epoch 38 | Batch 50/297 | Loss: 0.0033 | Time: 8.6s\n",
      "Epoch 38 | Batch 60/297 | Loss: 0.0031 | Time: 10.3s\n",
      "Epoch 38 | Batch 70/297 | Loss: 0.0032 | Time: 12.1s\n",
      "Epoch 38 | Batch 80/297 | Loss: 0.0034 | Time: 13.7s\n",
      "Epoch 38 | Batch 90/297 | Loss: 0.0034 | Time: 15.7s\n",
      "Epoch 38 | Batch 100/297 | Loss: 0.0034 | Time: 17.5s\n",
      "Epoch 38 | Batch 110/297 | Loss: 0.0033 | Time: 19.3s\n",
      "Epoch 38 | Batch 120/297 | Loss: 0.0035 | Time: 21.2s\n",
      "Epoch 38 | Batch 130/297 | Loss: 0.0035 | Time: 23.0s\n",
      "Epoch 38 | Batch 140/297 | Loss: 0.0035 | Time: 24.7s\n",
      "Epoch 38 | Batch 150/297 | Loss: 0.0034 | Time: 26.5s\n",
      "Epoch 38 | Batch 160/297 | Loss: 0.0035 | Time: 28.2s\n",
      "Epoch 38 | Batch 170/297 | Loss: 0.0035 | Time: 30.3s\n",
      "Epoch 38 | Batch 180/297 | Loss: 0.0035 | Time: 32.1s\n",
      "Epoch 38 | Batch 190/297 | Loss: 0.0036 | Time: 33.8s\n",
      "Epoch 38 | Batch 200/297 | Loss: 0.0036 | Time: 35.7s\n",
      "Epoch 38 | Batch 210/297 | Loss: 0.0036 | Time: 37.4s\n",
      "Epoch 38 | Batch 220/297 | Loss: 0.0037 | Time: 39.2s\n",
      "Epoch 38 | Batch 230/297 | Loss: 0.0036 | Time: 41.1s\n",
      "Epoch 38 | Batch 240/297 | Loss: 0.0036 | Time: 42.7s\n",
      "Epoch 38 | Batch 250/297 | Loss: 0.0035 | Time: 44.5s\n",
      "Epoch 38 | Batch 260/297 | Loss: 0.0035 | Time: 46.3s\n",
      "Epoch 38 | Batch 270/297 | Loss: 0.0035 | Time: 48.2s\n",
      "Epoch 38 | Batch 280/297 | Loss: 0.0035 | Time: 50.0s\n",
      "Epoch 38 | Batch 290/297 | Loss: 0.0034 | Time: 51.6s\n",
      "Train Loss: 0.0034\n",
      "Val Loss: 0.3257\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 39/50\n",
      "------------------------------------------------------------\n",
      "Epoch 39 | Batch 10/297 | Loss: 0.0022 | Time: 1.7s\n",
      "Epoch 39 | Batch 20/297 | Loss: 0.0022 | Time: 3.4s\n",
      "Epoch 39 | Batch 30/297 | Loss: 0.0021 | Time: 5.2s\n",
      "Epoch 39 | Batch 40/297 | Loss: 0.0024 | Time: 7.0s\n",
      "Epoch 39 | Batch 50/297 | Loss: 0.0029 | Time: 8.9s\n",
      "Epoch 39 | Batch 60/297 | Loss: 0.0028 | Time: 10.7s\n",
      "Epoch 39 | Batch 70/297 | Loss: 0.0029 | Time: 12.3s\n",
      "Epoch 39 | Batch 80/297 | Loss: 0.0029 | Time: 14.3s\n",
      "Epoch 39 | Batch 90/297 | Loss: 0.0029 | Time: 16.2s\n",
      "Epoch 39 | Batch 100/297 | Loss: 0.0029 | Time: 18.2s\n",
      "Epoch 39 | Batch 110/297 | Loss: 0.0028 | Time: 19.9s\n",
      "Epoch 39 | Batch 120/297 | Loss: 0.0028 | Time: 21.7s\n",
      "Epoch 39 | Batch 130/297 | Loss: 0.0029 | Time: 23.5s\n",
      "Epoch 39 | Batch 140/297 | Loss: 0.0029 | Time: 25.4s\n",
      "Epoch 39 | Batch 150/297 | Loss: 0.0028 | Time: 27.3s\n",
      "Epoch 39 | Batch 160/297 | Loss: 0.0028 | Time: 29.3s\n",
      "Epoch 39 | Batch 170/297 | Loss: 0.0030 | Time: 31.0s\n",
      "Epoch 39 | Batch 180/297 | Loss: 0.0031 | Time: 32.9s\n",
      "Epoch 39 | Batch 190/297 | Loss: 0.0031 | Time: 34.7s\n",
      "Epoch 39 | Batch 200/297 | Loss: 0.0031 | Time: 36.5s\n",
      "Epoch 39 | Batch 210/297 | Loss: 0.0031 | Time: 38.1s\n",
      "Epoch 39 | Batch 220/297 | Loss: 0.0031 | Time: 39.8s\n",
      "Epoch 39 | Batch 230/297 | Loss: 0.0031 | Time: 41.5s\n",
      "Epoch 39 | Batch 240/297 | Loss: 0.0030 | Time: 43.3s\n",
      "Epoch 39 | Batch 250/297 | Loss: 0.0030 | Time: 44.9s\n",
      "Epoch 39 | Batch 260/297 | Loss: 0.0030 | Time: 46.6s\n",
      "Epoch 39 | Batch 270/297 | Loss: 0.0030 | Time: 48.4s\n",
      "Epoch 39 | Batch 280/297 | Loss: 0.0030 | Time: 50.3s\n",
      "Epoch 39 | Batch 290/297 | Loss: 0.0030 | Time: 51.9s\n",
      "Train Loss: 0.0030\n",
      "Val Loss: 0.3259\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 40/50\n",
      "------------------------------------------------------------\n",
      "Epoch 40 | Batch 10/297 | Loss: 0.0027 | Time: 1.7s\n",
      "Epoch 40 | Batch 20/297 | Loss: 0.0029 | Time: 3.3s\n",
      "Epoch 40 | Batch 30/297 | Loss: 0.0033 | Time: 5.0s\n",
      "Epoch 40 | Batch 40/297 | Loss: 0.0036 | Time: 6.6s\n",
      "Epoch 40 | Batch 50/297 | Loss: 0.0039 | Time: 8.3s\n",
      "Epoch 40 | Batch 60/297 | Loss: 0.0039 | Time: 9.9s\n",
      "Epoch 40 | Batch 70/297 | Loss: 0.0037 | Time: 11.9s\n",
      "Epoch 40 | Batch 80/297 | Loss: 0.0035 | Time: 13.8s\n",
      "Epoch 40 | Batch 90/297 | Loss: 0.0034 | Time: 15.5s\n",
      "Epoch 40 | Batch 100/297 | Loss: 0.0033 | Time: 17.3s\n",
      "Epoch 40 | Batch 110/297 | Loss: 0.0034 | Time: 19.1s\n",
      "Epoch 40 | Batch 120/297 | Loss: 0.0033 | Time: 20.9s\n",
      "Epoch 40 | Batch 130/297 | Loss: 0.0033 | Time: 22.8s\n",
      "Epoch 40 | Batch 140/297 | Loss: 0.0032 | Time: 24.5s\n",
      "Epoch 40 | Batch 150/297 | Loss: 0.0032 | Time: 26.1s\n",
      "Epoch 40 | Batch 160/297 | Loss: 0.0031 | Time: 27.9s\n",
      "Epoch 40 | Batch 170/297 | Loss: 0.0030 | Time: 29.9s\n",
      "Epoch 40 | Batch 180/297 | Loss: 0.0030 | Time: 31.7s\n",
      "Epoch 40 | Batch 190/297 | Loss: 0.0030 | Time: 33.5s\n",
      "Epoch 40 | Batch 200/297 | Loss: 0.0030 | Time: 35.2s\n",
      "Epoch 40 | Batch 210/297 | Loss: 0.0031 | Time: 36.8s\n",
      "Epoch 40 | Batch 220/297 | Loss: 0.0031 | Time: 38.5s\n",
      "Epoch 40 | Batch 230/297 | Loss: 0.0031 | Time: 40.3s\n",
      "Epoch 40 | Batch 240/297 | Loss: 0.0031 | Time: 42.1s\n",
      "Epoch 40 | Batch 250/297 | Loss: 0.0031 | Time: 44.0s\n",
      "Epoch 40 | Batch 260/297 | Loss: 0.0031 | Time: 45.8s\n",
      "Epoch 40 | Batch 270/297 | Loss: 0.0031 | Time: 47.6s\n",
      "Epoch 40 | Batch 280/297 | Loss: 0.0030 | Time: 49.6s\n",
      "Epoch 40 | Batch 290/297 | Loss: 0.0031 | Time: 51.3s\n",
      "Train Loss: 0.0031\n",
      "Val Loss: 0.3261\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 41/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 | Batch 10/297 | Loss: 0.0026 | Time: 1.7s\n",
      "Epoch 41 | Batch 20/297 | Loss: 0.0027 | Time: 3.4s\n",
      "Epoch 41 | Batch 30/297 | Loss: 0.0036 | Time: 5.1s\n",
      "Epoch 41 | Batch 40/297 | Loss: 0.0037 | Time: 6.8s\n",
      "Epoch 41 | Batch 50/297 | Loss: 0.0042 | Time: 8.9s\n",
      "Epoch 41 | Batch 60/297 | Loss: 0.0040 | Time: 10.6s\n",
      "Epoch 41 | Batch 70/297 | Loss: 0.0039 | Time: 12.5s\n",
      "Epoch 41 | Batch 80/297 | Loss: 0.0038 | Time: 14.3s\n",
      "Epoch 41 | Batch 90/297 | Loss: 0.0036 | Time: 16.1s\n",
      "Epoch 41 | Batch 100/297 | Loss: 0.0038 | Time: 17.7s\n",
      "Epoch 41 | Batch 110/297 | Loss: 0.0037 | Time: 19.5s\n",
      "Epoch 41 | Batch 120/297 | Loss: 0.0037 | Time: 21.3s\n",
      "Epoch 41 | Batch 130/297 | Loss: 0.0036 | Time: 22.9s\n",
      "Epoch 41 | Batch 140/297 | Loss: 0.0038 | Time: 24.6s\n",
      "Epoch 41 | Batch 150/297 | Loss: 0.0037 | Time: 26.3s\n",
      "Epoch 41 | Batch 160/297 | Loss: 0.0039 | Time: 28.3s\n",
      "Epoch 41 | Batch 170/297 | Loss: 0.0038 | Time: 30.1s\n",
      "Epoch 41 | Batch 180/297 | Loss: 0.0039 | Time: 31.8s\n",
      "Epoch 41 | Batch 190/297 | Loss: 0.0039 | Time: 33.8s\n",
      "Epoch 41 | Batch 200/297 | Loss: 0.0039 | Time: 35.4s\n",
      "Epoch 41 | Batch 210/297 | Loss: 0.0038 | Time: 37.2s\n",
      "Epoch 41 | Batch 220/297 | Loss: 0.0038 | Time: 38.8s\n",
      "Epoch 41 | Batch 230/297 | Loss: 0.0038 | Time: 40.8s\n",
      "Epoch 41 | Batch 240/297 | Loss: 0.0038 | Time: 42.6s\n",
      "Epoch 41 | Batch 250/297 | Loss: 0.0037 | Time: 44.6s\n",
      "Epoch 41 | Batch 260/297 | Loss: 0.0037 | Time: 46.2s\n",
      "Epoch 41 | Batch 270/297 | Loss: 0.0037 | Time: 48.0s\n",
      "Epoch 41 | Batch 280/297 | Loss: 0.0037 | Time: 49.9s\n",
      "Epoch 41 | Batch 290/297 | Loss: 0.0037 | Time: 51.7s\n",
      "Train Loss: 0.0037\n",
      "Val Loss: 0.3262\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 42/50\n",
      "------------------------------------------------------------\n",
      "Epoch 42 | Batch 10/297 | Loss: 0.0029 | Time: 1.7s\n",
      "Epoch 42 | Batch 20/297 | Loss: 0.0029 | Time: 3.6s\n",
      "Epoch 42 | Batch 30/297 | Loss: 0.0029 | Time: 5.4s\n",
      "Epoch 42 | Batch 40/297 | Loss: 0.0026 | Time: 7.1s\n",
      "Epoch 42 | Batch 50/297 | Loss: 0.0026 | Time: 9.1s\n",
      "Epoch 42 | Batch 60/297 | Loss: 0.0026 | Time: 11.1s\n",
      "Epoch 42 | Batch 70/297 | Loss: 0.0027 | Time: 12.8s\n",
      "Epoch 42 | Batch 80/297 | Loss: 0.0027 | Time: 14.5s\n",
      "Epoch 42 | Batch 90/297 | Loss: 0.0027 | Time: 16.3s\n",
      "Epoch 42 | Batch 100/297 | Loss: 0.0029 | Time: 18.0s\n",
      "Epoch 42 | Batch 110/297 | Loss: 0.0030 | Time: 20.0s\n",
      "Epoch 42 | Batch 120/297 | Loss: 0.0029 | Time: 21.7s\n",
      "Epoch 42 | Batch 130/297 | Loss: 0.0029 | Time: 23.5s\n",
      "Epoch 42 | Batch 140/297 | Loss: 0.0029 | Time: 25.3s\n",
      "Epoch 42 | Batch 150/297 | Loss: 0.0029 | Time: 26.9s\n",
      "Epoch 42 | Batch 160/297 | Loss: 0.0028 | Time: 28.7s\n",
      "Epoch 42 | Batch 170/297 | Loss: 0.0030 | Time: 30.3s\n",
      "Epoch 42 | Batch 180/297 | Loss: 0.0030 | Time: 32.2s\n",
      "Epoch 42 | Batch 190/297 | Loss: 0.0030 | Time: 33.8s\n",
      "Epoch 42 | Batch 200/297 | Loss: 0.0030 | Time: 35.5s\n",
      "Epoch 42 | Batch 210/297 | Loss: 0.0030 | Time: 37.3s\n",
      "Epoch 42 | Batch 220/297 | Loss: 0.0030 | Time: 39.2s\n",
      "Epoch 42 | Batch 230/297 | Loss: 0.0031 | Time: 41.0s\n",
      "Epoch 42 | Batch 240/297 | Loss: 0.0030 | Time: 42.7s\n",
      "Epoch 42 | Batch 250/297 | Loss: 0.0031 | Time: 44.3s\n",
      "Epoch 42 | Batch 260/297 | Loss: 0.0030 | Time: 46.1s\n",
      "Epoch 42 | Batch 270/297 | Loss: 0.0030 | Time: 47.7s\n",
      "Epoch 42 | Batch 280/297 | Loss: 0.0030 | Time: 49.5s\n",
      "Epoch 42 | Batch 290/297 | Loss: 0.0031 | Time: 51.2s\n",
      "Train Loss: 0.0031\n",
      "Val Loss: 0.3262\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 43/50\n",
      "------------------------------------------------------------\n",
      "Epoch 43 | Batch 10/297 | Loss: 0.0035 | Time: 1.8s\n",
      "Epoch 43 | Batch 20/297 | Loss: 0.0036 | Time: 3.7s\n",
      "Epoch 43 | Batch 30/297 | Loss: 0.0035 | Time: 5.3s\n",
      "Epoch 43 | Batch 40/297 | Loss: 0.0036 | Time: 7.3s\n",
      "Epoch 43 | Batch 50/297 | Loss: 0.0036 | Time: 9.0s\n",
      "Epoch 43 | Batch 60/297 | Loss: 0.0035 | Time: 10.7s\n",
      "Epoch 43 | Batch 70/297 | Loss: 0.0034 | Time: 12.5s\n",
      "Epoch 43 | Batch 80/297 | Loss: 0.0035 | Time: 14.3s\n",
      "Epoch 43 | Batch 90/297 | Loss: 0.0037 | Time: 16.0s\n",
      "Epoch 43 | Batch 100/297 | Loss: 0.0035 | Time: 17.7s\n",
      "Epoch 43 | Batch 110/297 | Loss: 0.0035 | Time: 19.6s\n",
      "Epoch 43 | Batch 120/297 | Loss: 0.0034 | Time: 21.3s\n",
      "Epoch 43 | Batch 130/297 | Loss: 0.0033 | Time: 23.1s\n",
      "Epoch 43 | Batch 140/297 | Loss: 0.0034 | Time: 24.9s\n",
      "Epoch 43 | Batch 150/297 | Loss: 0.0033 | Time: 26.6s\n",
      "Epoch 43 | Batch 160/297 | Loss: 0.0034 | Time: 28.3s\n",
      "Epoch 43 | Batch 170/297 | Loss: 0.0033 | Time: 30.1s\n",
      "Epoch 43 | Batch 180/297 | Loss: 0.0032 | Time: 32.1s\n",
      "Epoch 43 | Batch 190/297 | Loss: 0.0033 | Time: 34.0s\n",
      "Epoch 43 | Batch 200/297 | Loss: 0.0033 | Time: 35.9s\n",
      "Epoch 43 | Batch 210/297 | Loss: 0.0033 | Time: 37.7s\n",
      "Epoch 43 | Batch 220/297 | Loss: 0.0033 | Time: 39.4s\n",
      "Epoch 43 | Batch 230/297 | Loss: 0.0033 | Time: 41.1s\n",
      "Epoch 43 | Batch 240/297 | Loss: 0.0033 | Time: 43.0s\n",
      "Epoch 43 | Batch 250/297 | Loss: 0.0033 | Time: 44.7s\n",
      "Epoch 43 | Batch 260/297 | Loss: 0.0033 | Time: 46.7s\n",
      "Epoch 43 | Batch 270/297 | Loss: 0.0033 | Time: 48.5s\n",
      "Epoch 43 | Batch 280/297 | Loss: 0.0033 | Time: 50.2s\n",
      "Epoch 43 | Batch 290/297 | Loss: 0.0034 | Time: 51.8s\n",
      "Train Loss: 0.0035\n",
      "Val Loss: 0.3260\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 44/50\n",
      "------------------------------------------------------------\n",
      "Epoch 44 | Batch 10/297 | Loss: 0.0023 | Time: 1.8s\n",
      "Epoch 44 | Batch 20/297 | Loss: 0.0030 | Time: 3.7s\n",
      "Epoch 44 | Batch 30/297 | Loss: 0.0033 | Time: 5.7s\n",
      "Epoch 44 | Batch 40/297 | Loss: 0.0033 | Time: 7.3s\n",
      "Epoch 44 | Batch 50/297 | Loss: 0.0030 | Time: 9.1s\n",
      "Epoch 44 | Batch 60/297 | Loss: 0.0028 | Time: 10.9s\n",
      "Epoch 44 | Batch 70/297 | Loss: 0.0027 | Time: 12.6s\n",
      "Epoch 44 | Batch 80/297 | Loss: 0.0026 | Time: 14.6s\n",
      "Epoch 44 | Batch 90/297 | Loss: 0.0028 | Time: 16.2s\n",
      "Epoch 44 | Batch 100/297 | Loss: 0.0027 | Time: 18.1s\n",
      "Epoch 44 | Batch 110/297 | Loss: 0.0027 | Time: 19.8s\n",
      "Epoch 44 | Batch 120/297 | Loss: 0.0027 | Time: 21.8s\n",
      "Epoch 44 | Batch 130/297 | Loss: 0.0027 | Time: 23.4s\n",
      "Epoch 44 | Batch 140/297 | Loss: 0.0029 | Time: 25.2s\n",
      "Epoch 44 | Batch 150/297 | Loss: 0.0029 | Time: 27.0s\n",
      "Epoch 44 | Batch 160/297 | Loss: 0.0029 | Time: 28.8s\n",
      "Epoch 44 | Batch 170/297 | Loss: 0.0029 | Time: 30.5s\n",
      "Epoch 44 | Batch 180/297 | Loss: 0.0029 | Time: 32.1s\n",
      "Epoch 44 | Batch 190/297 | Loss: 0.0029 | Time: 34.0s\n",
      "Epoch 44 | Batch 200/297 | Loss: 0.0030 | Time: 35.8s\n",
      "Epoch 44 | Batch 210/297 | Loss: 0.0030 | Time: 37.6s\n",
      "Epoch 44 | Batch 220/297 | Loss: 0.0030 | Time: 39.3s\n",
      "Epoch 44 | Batch 230/297 | Loss: 0.0031 | Time: 41.1s\n",
      "Epoch 44 | Batch 240/297 | Loss: 0.0031 | Time: 43.1s\n",
      "Epoch 44 | Batch 250/297 | Loss: 0.0030 | Time: 45.0s\n",
      "Epoch 44 | Batch 260/297 | Loss: 0.0030 | Time: 46.9s\n",
      "Epoch 44 | Batch 270/297 | Loss: 0.0031 | Time: 48.4s\n",
      "Epoch 44 | Batch 280/297 | Loss: 0.0031 | Time: 50.1s\n",
      "Epoch 44 | Batch 290/297 | Loss: 0.0031 | Time: 51.8s\n",
      "Train Loss: 0.0031\n",
      "Val Loss: 0.3260\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 45/50\n",
      "------------------------------------------------------------\n",
      "Epoch 45 | Batch 10/297 | Loss: 0.0014 | Time: 2.0s\n",
      "Epoch 45 | Batch 20/297 | Loss: 0.0023 | Time: 3.7s\n",
      "Epoch 45 | Batch 30/297 | Loss: 0.0030 | Time: 5.6s\n",
      "Epoch 45 | Batch 40/297 | Loss: 0.0030 | Time: 7.4s\n",
      "Epoch 45 | Batch 50/297 | Loss: 0.0029 | Time: 9.1s\n",
      "Epoch 45 | Batch 60/297 | Loss: 0.0030 | Time: 10.8s\n",
      "Epoch 45 | Batch 70/297 | Loss: 0.0029 | Time: 12.7s\n",
      "Epoch 45 | Batch 80/297 | Loss: 0.0030 | Time: 14.4s\n",
      "Epoch 45 | Batch 90/297 | Loss: 0.0030 | Time: 16.2s\n",
      "Epoch 45 | Batch 100/297 | Loss: 0.0032 | Time: 17.8s\n",
      "Epoch 45 | Batch 110/297 | Loss: 0.0032 | Time: 19.6s\n",
      "Epoch 45 | Batch 120/297 | Loss: 0.0031 | Time: 21.4s\n",
      "Epoch 45 | Batch 130/297 | Loss: 0.0032 | Time: 23.3s\n",
      "Epoch 45 | Batch 140/297 | Loss: 0.0032 | Time: 25.0s\n",
      "Epoch 45 | Batch 150/297 | Loss: 0.0032 | Time: 26.8s\n",
      "Epoch 45 | Batch 160/297 | Loss: 0.0033 | Time: 28.7s\n",
      "Epoch 45 | Batch 170/297 | Loss: 0.0033 | Time: 30.4s\n",
      "Epoch 45 | Batch 180/297 | Loss: 0.0032 | Time: 32.2s\n",
      "Epoch 45 | Batch 190/297 | Loss: 0.0033 | Time: 33.9s\n",
      "Epoch 45 | Batch 200/297 | Loss: 0.0033 | Time: 35.7s\n",
      "Epoch 45 | Batch 210/297 | Loss: 0.0033 | Time: 37.5s\n",
      "Epoch 45 | Batch 220/297 | Loss: 0.0032 | Time: 39.3s\n",
      "Epoch 45 | Batch 230/297 | Loss: 0.0032 | Time: 41.0s\n",
      "Epoch 45 | Batch 240/297 | Loss: 0.0032 | Time: 42.6s\n",
      "Epoch 45 | Batch 250/297 | Loss: 0.0032 | Time: 44.5s\n",
      "Epoch 45 | Batch 260/297 | Loss: 0.0032 | Time: 46.2s\n",
      "Epoch 45 | Batch 270/297 | Loss: 0.0032 | Time: 47.9s\n",
      "Epoch 45 | Batch 280/297 | Loss: 0.0032 | Time: 49.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 | Batch 290/297 | Loss: 0.0032 | Time: 51.4s\n",
      "Train Loss: 0.0032\n",
      "Val Loss: 0.3260\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 46/50\n",
      "------------------------------------------------------------\n",
      "Epoch 46 | Batch 10/297 | Loss: 0.0028 | Time: 2.1s\n",
      "Epoch 46 | Batch 20/297 | Loss: 0.0030 | Time: 3.8s\n",
      "Epoch 46 | Batch 30/297 | Loss: 0.0025 | Time: 5.5s\n",
      "Epoch 46 | Batch 40/297 | Loss: 0.0030 | Time: 7.2s\n",
      "Epoch 46 | Batch 50/297 | Loss: 0.0030 | Time: 9.2s\n",
      "Epoch 46 | Batch 60/297 | Loss: 0.0032 | Time: 10.9s\n",
      "Epoch 46 | Batch 70/297 | Loss: 0.0034 | Time: 12.8s\n",
      "Epoch 46 | Batch 80/297 | Loss: 0.0032 | Time: 14.3s\n",
      "Epoch 46 | Batch 90/297 | Loss: 0.0032 | Time: 16.1s\n",
      "Epoch 46 | Batch 100/297 | Loss: 0.0033 | Time: 17.8s\n",
      "Epoch 46 | Batch 110/297 | Loss: 0.0031 | Time: 19.5s\n",
      "Epoch 46 | Batch 120/297 | Loss: 0.0032 | Time: 21.2s\n",
      "Epoch 46 | Batch 130/297 | Loss: 0.0033 | Time: 23.1s\n",
      "Epoch 46 | Batch 140/297 | Loss: 0.0032 | Time: 24.8s\n",
      "Epoch 46 | Batch 150/297 | Loss: 0.0033 | Time: 26.6s\n",
      "Epoch 46 | Batch 160/297 | Loss: 0.0033 | Time: 28.4s\n",
      "Epoch 46 | Batch 170/297 | Loss: 0.0034 | Time: 30.1s\n",
      "Epoch 46 | Batch 180/297 | Loss: 0.0035 | Time: 31.9s\n",
      "Epoch 46 | Batch 190/297 | Loss: 0.0034 | Time: 33.6s\n",
      "Epoch 46 | Batch 200/297 | Loss: 0.0034 | Time: 35.5s\n",
      "Epoch 46 | Batch 210/297 | Loss: 0.0033 | Time: 37.2s\n",
      "Epoch 46 | Batch 220/297 | Loss: 0.0033 | Time: 39.0s\n",
      "Epoch 46 | Batch 230/297 | Loss: 0.0033 | Time: 40.9s\n",
      "Epoch 46 | Batch 240/297 | Loss: 0.0033 | Time: 42.8s\n",
      "Epoch 46 | Batch 250/297 | Loss: 0.0033 | Time: 44.7s\n",
      "Epoch 46 | Batch 260/297 | Loss: 0.0033 | Time: 46.5s\n",
      "Epoch 46 | Batch 270/297 | Loss: 0.0032 | Time: 48.2s\n",
      "Epoch 46 | Batch 280/297 | Loss: 0.0032 | Time: 50.1s\n",
      "Epoch 46 | Batch 290/297 | Loss: 0.0032 | Time: 51.6s\n",
      "Train Loss: 0.0032\n",
      "Val Loss: 0.3261\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 47/50\n",
      "------------------------------------------------------------\n",
      "Epoch 47 | Batch 10/297 | Loss: 0.0025 | Time: 1.6s\n",
      "Epoch 47 | Batch 20/297 | Loss: 0.0027 | Time: 3.5s\n",
      "Epoch 47 | Batch 30/297 | Loss: 0.0026 | Time: 5.3s\n",
      "Epoch 47 | Batch 40/297 | Loss: 0.0027 | Time: 7.1s\n",
      "Epoch 47 | Batch 50/297 | Loss: 0.0030 | Time: 8.7s\n",
      "Epoch 47 | Batch 60/297 | Loss: 0.0030 | Time: 10.4s\n",
      "Epoch 47 | Batch 70/297 | Loss: 0.0028 | Time: 12.2s\n",
      "Epoch 47 | Batch 80/297 | Loss: 0.0027 | Time: 14.1s\n",
      "Epoch 47 | Batch 90/297 | Loss: 0.0027 | Time: 15.9s\n",
      "Epoch 47 | Batch 100/297 | Loss: 0.0026 | Time: 17.7s\n",
      "Epoch 47 | Batch 110/297 | Loss: 0.0025 | Time: 19.3s\n",
      "Epoch 47 | Batch 120/297 | Loss: 0.0026 | Time: 21.0s\n",
      "Epoch 47 | Batch 130/297 | Loss: 0.0027 | Time: 22.7s\n",
      "Epoch 47 | Batch 140/297 | Loss: 0.0026 | Time: 24.5s\n",
      "Epoch 47 | Batch 150/297 | Loss: 0.0027 | Time: 26.3s\n",
      "Epoch 47 | Batch 160/297 | Loss: 0.0027 | Time: 28.1s\n",
      "Epoch 47 | Batch 170/297 | Loss: 0.0027 | Time: 29.8s\n",
      "Epoch 47 | Batch 180/297 | Loss: 0.0028 | Time: 31.5s\n",
      "Epoch 47 | Batch 190/297 | Loss: 0.0029 | Time: 33.3s\n",
      "Epoch 47 | Batch 200/297 | Loss: 0.0029 | Time: 35.2s\n",
      "Epoch 47 | Batch 210/297 | Loss: 0.0029 | Time: 37.1s\n",
      "Epoch 47 | Batch 220/297 | Loss: 0.0029 | Time: 38.9s\n",
      "Epoch 47 | Batch 230/297 | Loss: 0.0030 | Time: 40.8s\n",
      "Epoch 47 | Batch 240/297 | Loss: 0.0030 | Time: 42.5s\n",
      "Epoch 47 | Batch 250/297 | Loss: 0.0029 | Time: 44.2s\n",
      "Epoch 47 | Batch 260/297 | Loss: 0.0031 | Time: 45.8s\n",
      "Epoch 47 | Batch 270/297 | Loss: 0.0031 | Time: 47.5s\n",
      "Epoch 47 | Batch 280/297 | Loss: 0.0031 | Time: 49.4s\n",
      "Epoch 47 | Batch 290/297 | Loss: 0.0031 | Time: 51.3s\n",
      "Train Loss: 0.0031\n",
      "Val Loss: 0.3260\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 48/50\n",
      "------------------------------------------------------------\n",
      "Epoch 48 | Batch 10/297 | Loss: 0.0045 | Time: 1.8s\n",
      "Epoch 48 | Batch 20/297 | Loss: 0.0033 | Time: 3.5s\n",
      "Epoch 48 | Batch 30/297 | Loss: 0.0035 | Time: 5.3s\n",
      "Epoch 48 | Batch 40/297 | Loss: 0.0034 | Time: 7.4s\n",
      "Epoch 48 | Batch 50/297 | Loss: 0.0033 | Time: 9.2s\n",
      "Epoch 48 | Batch 60/297 | Loss: 0.0035 | Time: 11.2s\n",
      "Epoch 48 | Batch 70/297 | Loss: 0.0035 | Time: 12.9s\n",
      "Epoch 48 | Batch 80/297 | Loss: 0.0037 | Time: 14.6s\n",
      "Epoch 48 | Batch 90/297 | Loss: 0.0038 | Time: 16.3s\n",
      "Epoch 48 | Batch 100/297 | Loss: 0.0038 | Time: 18.1s\n",
      "Epoch 48 | Batch 110/297 | Loss: 0.0037 | Time: 19.9s\n",
      "Epoch 48 | Batch 120/297 | Loss: 0.0036 | Time: 21.7s\n",
      "Epoch 48 | Batch 130/297 | Loss: 0.0035 | Time: 23.5s\n",
      "Epoch 48 | Batch 140/297 | Loss: 0.0034 | Time: 25.3s\n",
      "Epoch 48 | Batch 150/297 | Loss: 0.0034 | Time: 27.1s\n",
      "Epoch 48 | Batch 160/297 | Loss: 0.0033 | Time: 28.9s\n",
      "Epoch 48 | Batch 170/297 | Loss: 0.0033 | Time: 30.6s\n",
      "Epoch 48 | Batch 180/297 | Loss: 0.0033 | Time: 32.3s\n",
      "Epoch 48 | Batch 190/297 | Loss: 0.0032 | Time: 34.1s\n",
      "Epoch 48 | Batch 200/297 | Loss: 0.0032 | Time: 35.8s\n",
      "Epoch 48 | Batch 210/297 | Loss: 0.0032 | Time: 37.4s\n",
      "Epoch 48 | Batch 220/297 | Loss: 0.0032 | Time: 39.4s\n",
      "Epoch 48 | Batch 230/297 | Loss: 0.0032 | Time: 41.1s\n",
      "Epoch 48 | Batch 240/297 | Loss: 0.0031 | Time: 43.1s\n",
      "Epoch 48 | Batch 250/297 | Loss: 0.0031 | Time: 44.8s\n",
      "Epoch 48 | Batch 260/297 | Loss: 0.0031 | Time: 46.6s\n",
      "Epoch 48 | Batch 270/297 | Loss: 0.0031 | Time: 48.2s\n",
      "Epoch 48 | Batch 280/297 | Loss: 0.0031 | Time: 50.1s\n",
      "Epoch 48 | Batch 290/297 | Loss: 0.0031 | Time: 51.8s\n",
      "Train Loss: 0.0030\n",
      "Val Loss: 0.3260\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 49/50\n",
      "------------------------------------------------------------\n",
      "Epoch 49 | Batch 10/297 | Loss: 0.0040 | Time: 1.7s\n",
      "Epoch 49 | Batch 20/297 | Loss: 0.0041 | Time: 3.4s\n",
      "Epoch 49 | Batch 30/297 | Loss: 0.0040 | Time: 5.2s\n",
      "Epoch 49 | Batch 40/297 | Loss: 0.0043 | Time: 7.2s\n",
      "Epoch 49 | Batch 50/297 | Loss: 0.0043 | Time: 9.0s\n",
      "Epoch 49 | Batch 60/297 | Loss: 0.0041 | Time: 10.7s\n",
      "Epoch 49 | Batch 70/297 | Loss: 0.0044 | Time: 12.6s\n",
      "Epoch 49 | Batch 80/297 | Loss: 0.0042 | Time: 14.6s\n",
      "Epoch 49 | Batch 90/297 | Loss: 0.0042 | Time: 16.6s\n",
      "Epoch 49 | Batch 100/297 | Loss: 0.0041 | Time: 18.4s\n",
      "Epoch 49 | Batch 110/297 | Loss: 0.0040 | Time: 20.2s\n",
      "Epoch 49 | Batch 120/297 | Loss: 0.0039 | Time: 22.0s\n",
      "Epoch 49 | Batch 130/297 | Loss: 0.0039 | Time: 23.7s\n",
      "Epoch 49 | Batch 140/297 | Loss: 0.0038 | Time: 25.5s\n",
      "Epoch 49 | Batch 150/297 | Loss: 0.0038 | Time: 27.4s\n",
      "Epoch 49 | Batch 160/297 | Loss: 0.0038 | Time: 29.1s\n",
      "Epoch 49 | Batch 170/297 | Loss: 0.0037 | Time: 30.9s\n",
      "Epoch 49 | Batch 180/297 | Loss: 0.0037 | Time: 32.6s\n",
      "Epoch 49 | Batch 190/297 | Loss: 0.0036 | Time: 34.3s\n",
      "Epoch 49 | Batch 200/297 | Loss: 0.0036 | Time: 36.1s\n",
      "Epoch 49 | Batch 210/297 | Loss: 0.0036 | Time: 37.7s\n",
      "Epoch 49 | Batch 220/297 | Loss: 0.0036 | Time: 39.4s\n",
      "Epoch 49 | Batch 230/297 | Loss: 0.0036 | Time: 41.2s\n",
      "Epoch 49 | Batch 240/297 | Loss: 0.0035 | Time: 43.0s\n",
      "Epoch 49 | Batch 250/297 | Loss: 0.0035 | Time: 44.7s\n",
      "Epoch 49 | Batch 260/297 | Loss: 0.0035 | Time: 46.2s\n",
      "Epoch 49 | Batch 270/297 | Loss: 0.0034 | Time: 48.1s\n",
      "Epoch 49 | Batch 280/297 | Loss: 0.0034 | Time: 49.9s\n",
      "Epoch 49 | Batch 290/297 | Loss: 0.0034 | Time: 51.7s\n",
      "Train Loss: 0.0034\n",
      "Val Loss: 0.3261\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "Epoch 50/50\n",
      "------------------------------------------------------------\n",
      "Epoch 50 | Batch 10/297 | Loss: 0.0040 | Time: 1.5s\n",
      "Epoch 50 | Batch 20/297 | Loss: 0.0031 | Time: 3.3s\n",
      "Epoch 50 | Batch 30/297 | Loss: 0.0036 | Time: 4.9s\n",
      "Epoch 50 | Batch 40/297 | Loss: 0.0033 | Time: 6.8s\n",
      "Epoch 50 | Batch 50/297 | Loss: 0.0034 | Time: 8.5s\n",
      "Epoch 50 | Batch 60/297 | Loss: 0.0032 | Time: 10.2s\n",
      "Epoch 50 | Batch 70/297 | Loss: 0.0032 | Time: 12.0s\n",
      "Epoch 50 | Batch 80/297 | Loss: 0.0030 | Time: 13.8s\n",
      "Epoch 50 | Batch 90/297 | Loss: 0.0030 | Time: 15.7s\n",
      "Epoch 50 | Batch 100/297 | Loss: 0.0031 | Time: 17.4s\n",
      "Epoch 50 | Batch 110/297 | Loss: 0.0030 | Time: 19.3s\n",
      "Epoch 50 | Batch 120/297 | Loss: 0.0030 | Time: 21.3s\n",
      "Epoch 50 | Batch 130/297 | Loss: 0.0030 | Time: 23.2s\n",
      "Epoch 50 | Batch 140/297 | Loss: 0.0031 | Time: 24.9s\n",
      "Epoch 50 | Batch 150/297 | Loss: 0.0031 | Time: 26.7s\n",
      "Epoch 50 | Batch 160/297 | Loss: 0.0032 | Time: 28.6s\n",
      "Epoch 50 | Batch 170/297 | Loss: 0.0032 | Time: 30.3s\n",
      "Epoch 50 | Batch 180/297 | Loss: 0.0033 | Time: 32.0s\n",
      "Epoch 50 | Batch 190/297 | Loss: 0.0032 | Time: 33.9s\n",
      "Epoch 50 | Batch 200/297 | Loss: 0.0032 | Time: 35.7s\n",
      "Epoch 50 | Batch 210/297 | Loss: 0.0032 | Time: 37.6s\n",
      "Epoch 50 | Batch 220/297 | Loss: 0.0032 | Time: 39.4s\n",
      "Epoch 50 | Batch 230/297 | Loss: 0.0032 | Time: 41.1s\n",
      "Epoch 50 | Batch 240/297 | Loss: 0.0031 | Time: 42.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | Batch 250/297 | Loss: 0.0032 | Time: 44.7s\n",
      "Epoch 50 | Batch 260/297 | Loss: 0.0031 | Time: 46.3s\n",
      "Epoch 50 | Batch 270/297 | Loss: 0.0032 | Time: 47.9s\n",
      "Epoch 50 | Batch 280/297 | Loss: 0.0032 | Time: 49.8s\n",
      "Epoch 50 | Batch 290/297 | Loss: 0.0032 | Time: 51.4s\n",
      "Train Loss: 0.0032\n",
      "Val Loss: 0.3261\n",
      "Learning Rate: 0.000000\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE - Best val loss: 0.3219\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "checkpoint = torch.load(f\"{CONFIG['models_dir']}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, CONFIG[\"num_epochs\"] + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['num_epochs']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, CONFIG[\"device\"], epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss = evaluate(model, dev_loader, criterion, CONFIG[\"device\"])\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Schedule\n",
    "    scheduler.step(val_loss)\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Learning Rate: {lr:.6f}\")\n",
    "    \n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss\n",
    "        }, f\"{CONFIG['models_dir']}/best_model.pt\")\n",
    "        print(f\"✓ Saved best model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TRAINING COMPLETE - Best val loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9238a9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmr\\AppData\\Local\\Temp\\ipykernel_7676\\4128139803.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{CONFIG['models_dir']}/best_model.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "\n",
      "Generating predictions on dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved predictions to isharah_clean/predictions.csv\n",
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Sample 02_0032\n",
      "REF: انا رغبه كوب شراء\n",
      "PRED: انا رغبه كوب شراء\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0021\n",
      "REF: الان واحد شهر عشر\n",
      "PRED: الان واحد شهر عشر\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0022\n",
      "REF: انا ذهاب مدرسه سبب كره_قدم\n",
      "PRED: انا ذهاب مدرسه سبب كره_قدم\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0004\n",
      "REF: هو معلم لا انا مدرسه\n",
      "PRED: انا ذهاب صيدليه مع اب\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0030\n",
      "REF: انا رغبه ذهاب لوحه بقاله\n",
      "PRED: انا رغبه ذهاب لوحه بقاله\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0008\n",
      "REF: انا اسره رقم ثلاث اشخاص\n",
      "PRED: اسره ام اب اخ ولد ثلاث\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0028\n",
      "REF: انا رغبه ذهاب مكتبه بعد صيف\n",
      "PRED: انا رغبه ذهاب مكتبه بعد صيف\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0007\n",
      "REF: استفهام هو صديق مدرسه\n",
      "PRED: هو عوده مع هو\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0016\n",
      "REF: هو معرفه ام لغه اشاره\n",
      "PRED: هو معرفه ام لغه اشاره\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0002\n",
      "REF: هو معلم لغه اشاره\n",
      "PRED: هو كتابه تاريخ الان\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "Exact Match Accuracy: 75.03% (712/949)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint = torch.load(f\"{CONFIG['models_dir']}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Inference function\n",
    "def decode_predictions(model, dataloader, vocab, sos_idx, eos_idx, device):\n",
    "    \"\"\"Generate predictions for entire dataset.\"\"\"\n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "    all_ids = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "        src = batch[\"src\"].to(device)\n",
    "        src_lengths = batch[\"src_lengths\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "        \n",
    "        # Create source mask\n",
    "        src_key_padding_mask = (\n",
    "            torch.arange(src.size(1), device=device).unsqueeze(0) >= src_lengths.unsqueeze(1)\n",
    "        )\n",
    "        \n",
    "        # Greedy decode\n",
    "        with torch.no_grad():\n",
    "            # Generate sequences\n",
    "            predictions = []\n",
    "            for i in range(src.size(0)):\n",
    "                pred = []\n",
    "                tgt_tokens = torch.full((1, 1), sos_idx, dtype=torch.long, device=device)\n",
    "                \n",
    "                for _ in range(100):  # max length\n",
    "                    logits = model(src[i:i+1], tgt_tokens, \n",
    "                                  src_key_padding_mask[i:i+1], None)\n",
    "                    next_token = logits[0, -1].argmax().item()\n",
    "                    \n",
    "                    if next_token == eos_idx:\n",
    "                        break\n",
    "                    pred.append(next_token)\n",
    "                    tgt_tokens = torch.cat([tgt_tokens, \n",
    "                                           torch.tensor([[next_token]], device=device)], dim=1)\n",
    "                \n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # Decode to text\n",
    "        for i, pred_indices in enumerate(predictions):\n",
    "            # Prediction\n",
    "            pred_text = \" \".join([vocab[idx] for idx in pred_indices \n",
    "                                 if idx < len(vocab) and vocab[idx] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            \n",
    "            # Reference\n",
    "            ref_indices = tgt[i][tgt[i] != -100].cpu().tolist()\n",
    "            ref_text = \" \".join([vocab[idx] for idx in ref_indices \n",
    "                                if idx < len(vocab) and vocab[idx] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            \n",
    "            all_preds.append(pred_text)\n",
    "            all_refs.append(ref_text)\n",
    "            all_ids.append(batch[\"sample_ids\"][i])\n",
    "    \n",
    "    return all_ids, all_preds, all_refs\n",
    "\n",
    "# Generate predictions on dev set\n",
    "print(\"\\nGenerating predictions on dev set...\")\n",
    "sample_ids, predictions, references = decode_predictions(\n",
    "    model, dev_loader, dev_dataset.vocab, \n",
    "    dev_dataset.sos_idx, dev_dataset.eos_idx, CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    \"sample_id\": sample_ids,\n",
    "    \"reference\": references,\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "results_df.to_csv(f\"{CONFIG['output_dir']}/predictions.csv\", index=False, encoding='utf-8')\n",
    "print(f\"✓ Saved predictions to {CONFIG['output_dir']}/predictions.csv\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(10, len(results_df))):\n",
    "    print(f\"\\nSample {results_df.iloc[i]['sample_id']}\")\n",
    "    print(f\"REF: {results_df.iloc[i]['reference']}\")\n",
    "    print(f\"PRED: {results_df.iloc[i]['prediction']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Calculate accuracy (exact match)\n",
    "exact_matches = (results_df[\"reference\"] == results_df[\"prediction\"]).sum()\n",
    "accuracy = exact_matches / len(results_df) * 100\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Exact Match Accuracy: {accuracy:.2f}% ({exact_matches}/{len(results_df)})\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bafc5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src, src_mask, sos_idx, eos_idx, device, beam_size=10, max_len=100):\n",
    "    \"\"\"\n",
    "    src: (1, T)\n",
    "    src_mask: (1, T)\n",
    "    \"\"\"\n",
    "    # Each beam entry: (log_prob, sequence_tensor)\n",
    "    beams = [(0.0, torch.tensor([[sos_idx]], dtype=torch.long, device=device))]\n",
    "\n",
    "    finished = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for log_prob, seq in beams:\n",
    "            # Stop if already ended\n",
    "            if seq[0, -1].item() == eos_idx:\n",
    "                finished.append((log_prob, seq))\n",
    "                continue\n",
    "\n",
    "            # Model forward\n",
    "            logits = model(src, seq, src_mask, None)  # shape: (1, L, vocab)\n",
    "            next_logits = logits[0, -1]               # last timestep\n",
    "            log_probs = torch.log_softmax(next_logits, dim=-1)\n",
    "\n",
    "            # Get top beam_size candidates\n",
    "            topk_log_probs, topk_ids = torch.topk(log_probs, beam_size)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                new_log_prob = log_prob + topk_log_probs[i].item()\n",
    "                new_seq = torch.cat([seq, topk_ids[i].view(1,1)], dim=1)\n",
    "                new_beams.append((new_log_prob, new_seq))\n",
    "\n",
    "        # Keep the top beams for the next step\n",
    "        new_beams.sort(key=lambda x: x[0], reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "        # If all beams ended → stop early\n",
    "        if len(finished) == beam_size:\n",
    "            break\n",
    "\n",
    "    # Combine finished + ongoing beams\n",
    "    all_candidates = finished + beams\n",
    "    all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return best sequence (tensor of shape (1, length))\n",
    "    return all_candidates[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6433a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engmr\\AppData\\Local\\Temp\\ipykernel_7676\\3997375661.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{CONFIG['models_dir']}/best_model.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "\n",
      "Generating predictions on dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|█████████████████████████████████████████████████████████| 30/30 [50:10<00:00, 100.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved predictions to isharah_clean/predictions.csv\n",
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Sample 02_0032\n",
      "REF: انا رغبه كوب شراء\n",
      "PRED: انا رغبه كوب شراء\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0021\n",
      "REF: الان واحد شهر عشر\n",
      "PRED: الان واحد شهر عشر\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0022\n",
      "REF: انا ذهاب مدرسه سبب كره_قدم\n",
      "PRED: انا ذهاب مدرسه سبب كره_قدم\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0004\n",
      "REF: هو معلم لا انا مدرسه\n",
      "PRED: انا ذهاب صيدليه مع اب\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0030\n",
      "REF: انا رغبه ذهاب لوحه بقاله\n",
      "PRED: انا رغبه ذهاب لوحه بقاله\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0008\n",
      "REF: انا اسره رقم ثلاث اشخاص\n",
      "PRED: اسره ام اب اخ ولد ثلاث\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0028\n",
      "REF: انا رغبه ذهاب مكتبه بعد صيف\n",
      "PRED: انا رغبه ذهاب مكتبه بعد صيف\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0007\n",
      "REF: استفهام هو صديق مدرسه\n",
      "PRED: هو عوده مع هو\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0016\n",
      "REF: هو معرفه ام لغه اشاره\n",
      "PRED: هو معرفه ام لغه اشاره\n",
      "----------------------------------------\n",
      "\n",
      "Sample 02_0002\n",
      "REF: هو معلم لغه اشاره\n",
      "PRED: هو كتابه تاريخ الان\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "Exact Match Accuracy: 75.24% (714/949)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint = torch.load(f\"{CONFIG['models_dir']}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Inference function\n",
    "def decode_predictions(model, dataloader, vocab, sos_idx, eos_idx, device):\n",
    "    \"\"\"Generate predictions for entire dataset.\"\"\"\n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "    all_ids = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "        src = batch[\"src\"].to(device)\n",
    "        src_lengths = batch[\"src_lengths\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "\n",
    "        # Create source mask\n",
    "        src_key_padding_mask = (\n",
    "            torch.arange(src.size(1), device=device).unsqueeze(0) >= src_lengths.unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        predictions = []   # <-- FIXED (this was missing)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Beam search for each sample in batch\n",
    "            for i in range(src.size(0)):\n",
    "                seq = beam_search(\n",
    "                    model,\n",
    "                    src[i:i+1],\n",
    "                    src_key_padding_mask[i:i+1],\n",
    "                    sos_idx,\n",
    "                    eos_idx,\n",
    "                    device,\n",
    "                    beam_size=10,\n",
    "                    max_len=100\n",
    "                )\n",
    "\n",
    "                # Convert tensor sequence to python list (skip <sos>)\n",
    "                pred_indices = seq[0, 1:].tolist()\n",
    "                predictions.append(pred_indices)\n",
    "\n",
    "        # Decode predictions + references\n",
    "        for i, pred_indices in enumerate(predictions):\n",
    "\n",
    "            # Decode prediction -> text\n",
    "            pred_text = \" \".join([\n",
    "                vocab[idx] for idx in pred_indices\n",
    "                if idx < len(vocab) and vocab[idx] not in [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "            ])\n",
    "\n",
    "            # Decode reference (ignore -100 because of CE loss)\n",
    "            ref_indices = tgt[i][tgt[i] != -100].cpu().tolist()\n",
    "            ref_text = \" \".join([\n",
    "                vocab[idx] for idx in ref_indices\n",
    "                if idx < len(vocab) and vocab[idx] not in [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "            ])\n",
    "\n",
    "            all_preds.append(pred_text)\n",
    "            all_refs.append(ref_text)\n",
    "            all_ids.append(batch[\"sample_ids\"][i])\n",
    "\n",
    "    return all_ids, all_preds, all_refs\n",
    "\n",
    "\n",
    "# Generate predictions on dev set\n",
    "print(\"\\nGenerating predictions on dev set...\")\n",
    "sample_ids, predictions, references = decode_predictions(\n",
    "    model, dev_loader, dev_dataset.vocab,\n",
    "    dev_dataset.sos_idx, dev_dataset.eos_idx, CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    \"sample_id\": sample_ids,\n",
    "    \"reference\": references,\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "results_df.to_csv(f\"{CONFIG['output_dir']}/predictions.csv\", index=False, encoding='utf-8')\n",
    "print(f\"✓ Saved predictions to {CONFIG['output_dir']}/predictions.csv\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(10, len(results_df))):\n",
    "    print(f\"\\nSample {results_df.iloc[i]['sample_id']}\")\n",
    "    print(f\"REF: {results_df.iloc[i]['reference']}\")\n",
    "    print(f\"PRED: {results_df.iloc[i]['prediction']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Calculate accuracy (exact match)\n",
    "exact_matches = (results_df[\"reference\"] == results_df[\"prediction\"]).sum()\n",
    "accuracy = exact_matches / len(results_df) * 100\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Exact Match Accuracy: {accuracy:.2f}% ({exact_matches}/{len(results_df)})\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7ceb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Loaded 3800 test samples\n",
      "\n",
      "Processing test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3800/3800 [00:08<00:00, 453.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 3800 test samples\n",
      "Test samples: 3800\n",
      "Test batches: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test pickle\n",
    "print(\"Loading test data...\")\n",
    "test_pickle_path = r\"C:\\Users\\engmr\\Downloads\\public_si_dat\\pose_data_isharah1000_SI_test.pkl\"\n",
    "with open(test_pickle_path, \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test samples\")\n",
    "\n",
    "# Process test features (same as train/dev preprocessing)\n",
    "def process_test_features(pickle_data, output_dir):\n",
    "    \"\"\"Process and save test features.\"\"\"\n",
    "    features_dir = Path(output_dir) / \"features\" / \"test\"\n",
    "    features_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    processed = []\n",
    "    print(\"\\nProcessing test features...\")\n",
    "    for sample_id in tqdm(pickle_data.keys()):\n",
    "        try:\n",
    "            keypoints = pickle_data[sample_id][\"keypoints\"]\n",
    "            features = normalize_keypoints(keypoints)\n",
    "            \n",
    "            # Save\n",
    "            feat_path = features_dir / f\"{sample_id}.npy\"\n",
    "            np.save(feat_path, features)\n",
    "            \n",
    "            processed.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"num_frames\": len(features)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sample_id}: {e}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    meta_df = pd.DataFrame(processed)\n",
    "    meta_path = Path(output_dir) / \"labels\" / \"test_metadata.csv\"\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    \n",
    "    print(f\"✓ Processed {len(processed)} test samples\")\n",
    "    return meta_df\n",
    "\n",
    "# Process test data\n",
    "test_meta = process_test_features(test_data, CONFIG[\"output_dir\"])\n",
    "\n",
    "# Create test dataset (no labels, so different structure)\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, features_dir, metadata_csv):\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.metadata = pd.read_csv(metadata_csv)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        sample_id = str(row[\"sample_id\"])\n",
    "        \n",
    "        feat_path = self.features_dir / f\"{sample_id}.npy\"\n",
    "        features = np.load(feat_path)\n",
    "        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"sample_id\": sample_id\n",
    "        }\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    \"\"\"Collate for test (no targets).\"\"\"\n",
    "    batch = sorted(batch, key=lambda x: len(x[\"features\"]), reverse=True)\n",
    "    \n",
    "    max_src_len = max(len(b[\"features\"]) for b in batch)\n",
    "    feat_dim = batch[0][\"features\"].shape[1]\n",
    "    \n",
    "    src_padded = np.zeros((len(batch), max_src_len, feat_dim), dtype=np.float32)\n",
    "    src_lengths = []\n",
    "    \n",
    "    for i, b in enumerate(batch):\n",
    "        seq_len = len(b[\"features\"])\n",
    "        src_padded[i, :seq_len, :] = b[\"features\"]\n",
    "        src_lengths.append(seq_len)\n",
    "    \n",
    "    return {\n",
    "        \"src\": torch.from_numpy(src_padded),\n",
    "        \"src_lengths\": torch.tensor(src_lengths, dtype=torch.long),\n",
    "        \"sample_ids\": [b[\"sample_id\"] for b in batch]\n",
    "    }\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = TestDataset(\n",
    "    CONFIG[\"features_dir\"] + \"/test\",\n",
    "    CONFIG[\"labels_dir\"] + \"/test_metadata.csv\"\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False, collate_fn=test_collate_fn, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Generate test predictions WITH PRESERVED ORDER\n",
    "def decode_test_predictions(model, dataset, vocab, sos_idx, eos_idx, device, beam_size=2):\n",
    "    \"\"\"Generate predictions for test set - PRESERVES ORIGINAL ORDER.\"\"\"\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Process ONE sample at a time to preserve order\n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Generating test predictions\"):\n",
    "        sample = dataset[idx]\n",
    "        sample_id = sample['sample_id']\n",
    "        \n",
    "        # Prepare input\n",
    "        features = torch.from_numpy(sample['features']).unsqueeze(0).to(device)\n",
    "        src_key_padding_mask = torch.zeros(1, features.size(1), dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Beam search\n",
    "        with torch.no_grad():\n",
    "            seq = beam_search(\n",
    "                model,\n",
    "                features,\n",
    "                src_key_padding_mask,\n",
    "                sos_idx,\n",
    "                eos_idx,\n",
    "                device,\n",
    "                beam_size=beam_size,\n",
    "                max_len=100\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        pred_indices = seq[0, 1:].tolist()\n",
    "        pred_text = \" \".join([\n",
    "            vocab[idx] for idx in pred_indices\n",
    "            if idx < len(vocab) and vocab[idx] not in [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "        ])\n",
    "        \n",
    "        all_ids.append(sample_id)\n",
    "        all_preds.append(pred_text)\n",
    "    \n",
    "    return all_ids, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecd15192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test predictions: 100%|███████████████████████████████████████████████| 3800/3800 [1:21:44<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 3800 predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "test_ids, test_predictions = decode_test_predictions(\n",
    "    model, test_dataset, dev_dataset.vocab,\n",
    "    dev_dataset.sos_idx, dev_dataset.eos_idx,\n",
    "    CONFIG[\"device\"], beam_size=5\n",
    ")\n",
    "\n",
    "# Save\n",
    "test_results = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"gloss\": test_predictions\n",
    "})\n",
    "test_results.to_csv(f\"{CONFIG['output_dir']}/test.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ Saved {len(test_results)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578751c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
